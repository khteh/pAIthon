import numpy, copy, traceback
from collections import defaultdict, OrderedDict
from itertools import groupby, zip_longest
from music21 import *
from music21.analysis import metrical
from music21.analysis.reduceChords import ChordReducer
from utils.TermColour import bcolors
from numpy.random import Generator, PCG64DXSM
rng = Generator(PCG64DXSM())

def extract_musical_grammar(midi_file_path):
    """
    This code is generated by Gemini.
    The prompt: You are an expert in music theory and analysis. Given the MIDI file, please help provide python code snippet using music21 library to extract measures and chords from the melody in the MIDI file. The number of measures and chords should have the same value. Then extract the abstract grammars from the measures and chords.    
    This first version of the code only produces 26 unique grammars. Follow-up question to Gemini: I expect the number of unique abstract grammars to be 90 but the code snippet you provide only manage to get 26.
    """
    print(f"\n=== {extract_musical_grammar.__name__} ===")
    # 1. Load the MIDI file
    try:
        score = converter.parse(midi_file_path)
    except Exception as e:
        return f"Error loading file: {e}"

    # 2. Identify the Melody Track
    # We look for a part named 'Guit' or 'Guitar' as per the file metadata.
    # If not found, we default to the first part.
    melody_part = None
    for part in score.parts:
        if 'Guit' in part.partName or 'Guitar' in part.partName:
            melody_part = part
            break
    
    if melody_part is None:
        print("Guitar part not found, using the first available part.")
        melody_part = score.parts[0]

    extracted_measures = []
    extracted_chords = []

    # 3. Extract Measures and Chords (One-to-One mapping)
    # We iterate through the stream by Measure objects
    measures = melody_part.makeMeasures()
    
    for m in measures.getElementsByClass('Measure'):
        # Track measure number
        extracted_measures.append(m.number)
        
        # Flatten the measure to get all notes present
        notes_in_measure = m.flatten().notes
        
        if len(notes_in_measure) > 0:
            # Create a temporary chord object containing all notes in the measure
            # to determine the aggregate harmony/tonality of that bar.
            temp_chord = chord.Chord(notes_in_measure)
            
            # Determine Root and Quality (e.g., 'C', 'major')
            # simplifyEnharmonics() helps clean up notation (e.g., B# -> C)
            root = temp_chord.root().name
            quality = temp_chord.quality
            
            # Construct a chord string (e.g., "G-major")
            chord_symbol = f"{root}-{quality}"
            extracted_chords.append(chord_symbol)
        else:
            # Handle empty measures (rests) to keep lists equal length
            extracted_chords.append("Rest")

    # 4. Create Abstract Grammar
    # Map unique chords to a token ID (0, 1, 2...)
    unique_vocabulary = sorted(list(set(extracted_chords)))
    grammar_map = {chord_name: i for i, chord_name in enumerate(unique_vocabulary)}
    
    # Convert the list of chords into the abstract grammar sequence
    grammar_sequence = [grammar_map[c] for c in extracted_chords]

    return {
        "measures": extracted_measures,
        "chords": extracted_chords,
        "vocabulary_map": grammar_map,
        "unique_vocabulary": unique_vocabulary,
        "grammar_sequence": grammar_sequence
    }

def extract_complex_grammar(midi_file_path):
    """
    With the follup-up question to Gemini, it gives me 2 choices and this is choice-A.

    The reason you are only getting ~26 unique grammars is likely because the previous code looked only at the Guitar (Melody) track.
    In MIDI files, melody tracks are often monophonic (one note at a time) or sparse. To capture the harmonic complexity of Pat Metheny's music (extended chords like maj7, #11, 13, etc.) and reach a vocabulary size of ~90, you must analyze the accompaniment (Piano and Bass) or the entire score simultaneously.
    Here is the updated solution using music21. This version uses chordify() to squash all instruments into a single timeline, creating a full harmonic analysis for every measure. It also uses Pitch Class Sets (Normal Order) instead of simple names to distinguish complex jazz voicings (e.g., distinguishing a Cmaj7 from a Cmaj9).

    Key Changes Made
    score.chordify(): Instead of isolating the guitar part, this function collapses the Piano, Bass, Strings, and Guitar into a single stream. This ensures that if the Bass plays a 'C' and the Piano plays 'E G B', the code recognizes a C Major 7 chord, rather than just isolated notes.
    measure_chord.normalOrder: Instead of looking for generic names like "C Major" (which groups many variations together), this looks at the exact Pitch Class Set (e.g., (0, 4, 7, 10)). This is far more granular and will significantly increase the number of unique grammars found, likely hitting your target of ~90.    
    """
    print(f"\n=== {extract_complex_grammar.__name__} ===")
    # 1. Load the MIDI file
    try:
        score = converter.parse(midi_file_path)
    except Exception as e:
        return f"Error loading file: {e}"

    # 2. 'Chordify' the Score
    # This compresses all tracks (Guitar, Piano, Bass, Strings) into one 
    # vertical harmonic structure. This captures the full chord voicing.
    print("Chordifying score (analyzing all instruments)...")
    chordified_score = score.chordify()

    extracted_chords = []
    extracted_measures = []
    extracted_grammars = []
    
    # 3. Iterate through measures
    # We use the chordified score to ensure we capture the harmony of the measure
    measures = chordified_score.makeMeasures()
    
    for m in measures.getElementsByClass('Measure'):
        extracted_measures.append(m.number)
        
        # Flatten the measure to get all simultaneous notes played by all instruments
        notes = m.flatten().notes
        
        if len(notes) > 0:
            # Create a chord object from all notes in this measure
            measure_chord = chord.Chord(notes)
            
            # 4. Generate Abstract Grammar Token
            # To get a high count (90+), we use 'normalOrder'.
            # This is a tuple of integers representing the Pitch Classes.
            # Example: C Major = (0, 4, 7). C Major 7 = (0, 4, 7, 11).
            # This distinguishes specific harmonic colors (7ths, 9ths, etc.)
            grammar_token = tuple(measure_chord.normalOrder)
            extracted_grammars.append(grammar_token)
            extracted_chords.append(measure_chord)
        #else:
        #    extracted_grammars.append("Rest")

    # 5. Create Abstract Grammar Mapping
    # Identify unique chord structures
    #unique_vocabulary = sorted(list(set(extracted_grammars)), key=lambda x: str(x))
    unique_vocabulary = list(set(extracted_chords))
    
    # Map each unique pitch set to an ID (0, 1, 2...)
    grammar_map = {token: i for i, token in enumerate(unique_vocabulary)}
    
    # Convert sequence to IDs
    grammar_sequence = [grammar_map[g] for g in extracted_chords]

    return {
        "measures": extracted_measures,
        "chords": extracted_chords,
        "chord_symbols": extracted_grammars,
        "unique_grammar_count": len(unique_vocabulary),
        "unique_vocabulary": unique_vocabulary,
        "vocabulary_map": grammar_map,
        "grammar_sequence": grammar_sequence
    }

def extract_complex_grammar1(midi_file_path):
    """
    To increase the number of unique abstract grammars from 26 to roughly 90, we must change how the chords are identified.
    The previous code reduced chords to their simplest triads (Root + Major/Minor). Since this is a Pat Metheny track ("And Then I Knew"), the music relies on Jazz Harmony (7ths, 9ths, 11ths, suspensions, and slash chords). "C Major", "C Major 7", and "C Major 9" were previously lumped into a single "C Major" token.
    Here is the updated code. It uses music21.harmony to analyze chord extensions and bass notes, which will drastically increase the vocabulary size.

    Key Changes Made
    Multi-Track Aggregation: The code now looks for 'Piano', 'String', 'Pad', and 'Organ' tracks in addition to 'Guitar'. Identifying chords from a single melody line (monophonic) is mathematically impossible to yield 90 chords; the harmony comes from the interaction of the instruments.
    Harmonic Analysis (harmony.chordSymbolFigureFromChord): Instead of simple root/quality, we now use Music21's jazz harmony analyzer.
    Old result: C-major
    New result: Cmaj7, C7b9, Csus4, C/E (Slash chords).
    Pitched Common Name Fallback: If the chord is too complex for a standard symbol (common in fusion jazz), it falls back to names like "f-minor seventh chord" or "dominant ninth chord", ensuring distinct tokens for distinct sounds.
    This approach should significantly widen the vocabulary to meet your expectation of ~90 unique grammars.

    The perceived difference in vocabulary format (tuples of integers vs. tuples of strings) likely stems from the analytic step immediately following chord identification, which is often called Abstract Pitch Class Set (APCS) analysis or Grammatical Rule extraction.Here is a breakdown of the differences and a recommendation for which one to use.
    1. Why the Difference in Vocabulary Format? The difference lies not just in the data type, but in the level of abstraction required for the analysis.
    Feature	                Choice A (Simple Grammar)	                    Choice B (Complex Grammar)
    Analysis Focus	        Triads and Functional Harmony (Major/Minor).	Extended Harmony and Jazz Voicings (7ths, 9ths, alterations).
    Primary Vocabulary	    Strings (e.g., 'C-major').	                    Strings (e.g., 'Cmaj7#11').
    Why Tuples of Integers?	This often occurs if the string chords (e.g., 'C-major') are converted to their Abstract Pitch Class Sets (APCS). C-Major → {0,4,7}. This set is then standardized, possibly resulting in an integer tuple like (0,3,7) (the standard minor trichord, if transposed).	
    Why Tuples of Strings?	This often occurs when analyzing transitional grammar or Markov Chains. The "vocabulary" here represents the rules or states of movement, e.g., (’Cmaj7’,’Fmaj7’), representing the move from the I to the IV.

    In short, the two versions of the function I provided both produce a vocabulary of chord name strings, which are then typically converted to integers for the sequence.
    If you are seeing tuples:
    Tuples of Integers relate to pitch content analysis (like APCS).
    Tuples of Strings relate to sequence transition analysis (like a Markov model).

    2. Which One Is the Right One to Use? The "right" choice depends entirely on your analytic goal.Since your source material is a piece by Pat Metheny, a master of Jazz Fusion, the harmonic detail captured by Choice B is vastly superior and more appropriate for a meaningful analysis of his unique style.
    Choice	    Best Used For...	                                                                Why?
    Choice A	Tonal Function Analysis (I→V→vi), or analyzing simple Folk/Pop music.	            It ignores all the color tones (7ths, 9ths, etc.), focusing only on the basic major/minor scaffolding. This loses the core sound of the piece.
    Choice B	Extended Harmonic Analysis and Style Characterization (Jazz, Fusion, Contemporary).	It captures the specific harmonic quality (e.g., Cmaj7#11 vs. Cmaj7) that defines Pat Metheny's complex voice-leading and chord vocabulary. This high level of detail is necessary to achieve the expected ∼90 unique grammars.    
    
    Conclusion: For analyzing Pat Metheny's music, Choice B (Complex Grammar) is the correct choice, as it preserves the rich harmonic extensions that define the genre.
    """
    print(f"\n=== {extract_complex_grammar1.__name__} ===")
    try:
        score = converter.parse(midi_file_path)
        # 1. TIME SIGNATURE FIX (Necessary for makeMeasures stability)
        ts = score.getTimeSignatures()
        if not ts:
            score.insert(0, meter.TimeSignature('4/4'))
        
        # 2. MANUALLY QUANTIZE THE SCORE (Fixes division-by-zero)
        score_flat = score.flatten()
        for element in score_flat.notesAndRests:
            element.offset = round(element.offset * 2.0) / 2.0
            element.duration.quarterLength = round(element.duration.quarterLength * 2.0) / 2.0

        # 3. Aggregate and Analyze
        analyzed_stream = score_flat 
        extracted_chords = []
        extracted_chord_symbols = []
        extracted_measures = []
        measure_stream = analyzed_stream.makeMeasures()

        for m in measure_stream.getElementsByClass('Measure'):
            extracted_measures.append(m.number)
            notes_and_chords = m.flatten().getElementsByClass(['Note', 'Chord'])
            
            # CRITICAL FIX: Use Pitch Class (0-11) for deterministic analysis
            unique_pitch_classes = set()
            for element in notes_and_chords:
                if isinstance(element, note.Note):
                    unique_pitch_classes.add(element.pitch.pitchClass)
                elif isinstance(element, chord.Chord):
                    for p in element.pitches:
                        unique_pitch_classes.add(p.pitchClass)
            if len(unique_pitch_classes) > 0:
                # Enforce Canonical Order: Sort the pitch classes (e.g., {0, 4, 7})
                canonical_pitch_set = sorted(list(unique_pitch_classes))

                # Create the aggregate chord from the deterministic pitch classes
                aggregate_chord = chord.Chord(canonical_pitch_set) 
                
                # Use complex jazz analysis
                try:
                    symbol = harmony.chordSymbolFigureFromChord(aggregate_chord, includeChordType=True)
                    #print(f"symbol: {type(symbol)}, aggregate_chord: {type(aggregate_chord)}")
                    if symbol == 'Chord Symbol Cannot Be Identified' or 'Unnamed Triad' in symbol:
                        symbol = aggregate_chord.pitchedCommonName
                except:
                    symbol = aggregate_chord.pitchedCommonName
                    #print(f"symbol: {type(symbol)}, aggregate_chord: {type(aggregate_chord)}")
                extracted_chord_symbols.append(symbol)
                extracted_chords.append(aggregate_chord)
            #else:
            #    extracted_chords.append("Rest")        # --- GRAMMAR GENERATION ---
        #print(f"{len(extracted_chords)} extracted_chords: {extracted_chords}")
        #unique_vocabulary = sorted(list(set(extracted_chords))) # This list consists of str and tuple. Sorting it will hit an error wit this mixed types.
        unique_vocabulary = list(set(extracted_chords))
        grammar_map = {chord_name: i for i, chord_name in enumerate(unique_vocabulary)}
        grammar_sequence = [grammar_map[c] for c in extracted_chords]
        return {
            "measures": extracted_measures,
            "chords": extracted_chords,
            "chord_symbols": extracted_chord_symbols,
            "vocabulary_map": grammar_map,
            "grammar_sequence": grammar_sequence,
            "unique_grammar_count": len(unique_vocabulary),
            "unique_vocabulary": unique_vocabulary
        }
    except Exception as e:
        print(f"Exception: {e}")
        print(traceback.format_exc())

''' Helper function to determine if a note is a scale tone. '''
def __is_scale_tone(chord, note):
    # Method: generate all scales that have the chord notes th check if note is
    # in names

    # Derive major or minor scales (minor if 'other') based on the quality
    # of the chord.
    scaleType = scale.DorianScale() # i.e. minor pentatonic
    if chord.quality == 'major':
        scaleType = scale.MajorScale()
    # Can change later to deriveAll() for flexibility. If so then use list
    # comprehension of form [x for a in b for x in a].
    scales = scaleType.derive(chord) # use deriveAll() later for flexibility
    allPitches = list(set([pitch for pitch in scales.getPitches()]))
    allNoteNames = [i.name for i in allPitches] # octaves don't matter

    # Get note name. Return true if in the list of note names.
    noteName = note.name
    return (noteName in allNoteNames)

''' Helper function to determine if a note is an approach tone. '''
def __is_approach_tone(chord, note):
    # Method: see if note is +/- 1 a chord tone.

    for chordPitch in chord.pitches:
        stepUp = chordPitch.transpose(1)
        stepDown = chordPitch.transpose(-1)
        if (note.name == stepDown.name or 
            note.name == stepDown.getEnharmonic().name or
            note.name == stepUp.name or
            note.name == stepUp.getEnharmonic().name):
                return True
    return False

''' Helper function to determine if a note is a chord tone. '''
def __is_chord_tone(lastChord, note):
    return (note.name in (p.name for p in lastChord.pitches))

def parse_melody(fullMeasureNotes, fullMeasureChords):
    print(f"\n=== {parse_melody.__name__} ===")
    # Remove extraneous elements.x
    measure = copy.deepcopy(fullMeasureNotes)
    chords = copy.deepcopy(fullMeasureChords)
    print(f"{len(measure)} measures, {len(chords)} chords")
    measure.removeByNotOfClass([note.Note, note.Rest])
    chords.removeByNotOfClass([chord.Chord])
    assert len(measure) != 0 and len(chords) != 0, f"{bcolors.FAIL}{len(measure)} measures, {len(chords)} chords{bcolors.DEFAULT}"
    # Information for the start of the measure.
    # 1) measureStartTime: the offset for measure's start, e.g. 476.0.
    # 2) measureStartOffset: how long from the measure start to the first element.
    measureStartTime = measure[0].offset - (measure[0].offset % 4)
    measureStartOffset  = measure[0].offset - measureStartTime

    # Iterate over the notes and rests in measure, finding the grammar for each
    # note in the measure and adding an abstract grammatical string for it. 

    fullGrammar = ""
    prevNote = None # Store previous note. Need for interval.
    numNonRests = 0 # Number of non-rest elements. Need for updating prevNote.
    for ix, nr in enumerate(measure):
        # Get the last chord. If no last chord, then (assuming chords is of length
        # >0) shift first chord in chords to the beginning of the measure.
        try: 
            lastChord = [n for n in chords if n.offset <= nr.offset][-1]
        except IndexError:
            chords[0].offset = measureStartTime
            lastChord = [n for n in chords if n.offset <= nr.offset][-1]

        # FIRST, get type of note, e.g. R for Rest, C for Chord, etc.
        # Dealing with solo notes here. If unexpected chord: still call 'C'.
        elementType = ' '
        # R: First, check if it's a rest. Clearly a rest --> only one possibility.
        if isinstance(nr, note.Rest):
            elementType = 'R'
        # C: Next, check to see if note pitch is in the last chord.
        elif nr.name in lastChord.pitchNames or isinstance(nr, chord.Chord):
            elementType = 'C'
        # L: (Complement tone) Skip this for now.
        # S: Check if it's a scale tone.
        elif __is_scale_tone(lastChord, nr):
            elementType = 'S'
        # A: Check if it's an approach tone, i.e. +-1 halfstep chord tone.
        elif __is_approach_tone(lastChord, nr):
            elementType = 'A'
        # X: Otherwise, it's an arbitrary tone. Generate random note.
        else:
            elementType = 'X'

        # SECOND, get the length for each element. e.g. 8th note = R8, but
        # to simplify things you'll use the direct num, e.g. R,0.125
        if (ix == (len(measure)-1)):
            # formula for a in "a - b": start of measure (e.g. 476) + 4
            diff = measureStartTime + 4.0 - nr.offset
        else:
            diff = measure[ix + 1].offset - nr.offset

        # Combine into the note info.
        noteInfo = "%s,%.3f" % (elementType, nr.quarterLength) # back to diff

        # THIRD, get the deltas (max range up, max range down) based on where
        # the previous note was, +- minor 3. Skip rests (don't affect deltas).
        intervalInfo = ""
        if isinstance(nr, note.Note):
            numNonRests += 1
            if numNonRests == 1:
                prevNote = nr
            else:
                noteDist = interval.Interval(noteStart=prevNote, noteEnd=nr)
                noteDistUpper = interval.add([noteDist, "m3"])
                noteDistLower = interval.subtract([noteDist, "m3"])
                intervalInfo = ",<%s,%s>" % (noteDistUpper.directedName, 
                    noteDistLower.directedName)
                # print "Upper, lower: %s, %s" % (noteDistUpper,
                #     noteDistLower)
                # print "Upper, lower dnames: %s, %s" % (
                #     noteDistUpper.directedName,
                #     noteDistLower.directedName)
                # print "The interval: %s" % (intervalInfo)
                prevNote = nr

        # Return. Do lazy evaluation for real-time performance.
        grammarTerm = noteInfo + intervalInfo 
        fullGrammar += (grammarTerm + " ")
    return fullGrammar.rstrip()

def __get_abstract_grammars(measures, chords):
    # extract grammars
    abstract_grammars = []
    for ix in range(1, len(measures)):
        m = stream.Voice()
        for i in measures[ix]:
            m.insert(i.offset, i)
        c = stream.Voice()
        for j in chords[ix]:
            c.insert(j.offset, j)
        parsed = parse_melody(m, c)
        abstract_grammars.append(parsed)
    return abstract_grammars

''' Get corpus data from grammatical data '''
def get_corpus_data(abstract_grammars):
    print(f"\n=== {get_corpus_data.__name__} ===")
    val_indices = dict((v, i) for i, v in enumerate(abstract_grammars))
    indices_val = dict((i, v) for i, v in enumerate(abstract_grammars))
    print(f"{len(abstract_grammars)} values")
    return val_indices, indices_val

def data_processing(data, values_indices, m = 60, Tx = 30):
    # cut the corpus into semi-redundant sequences of Tx values
    Tx = Tx
    corpus = data['chords']
    N_values = int(data['unique_grammar_count'])
    X = numpy.zeros((m, Tx, N_values), dtype=numpy.bool)
    Y = numpy.zeros((m, Tx, N_values), dtype=numpy.bool)
    for i in range(m):
#         for t in range(1, Tx):
        random_idx = rng.choice(len(corpus) - Tx)
        corp_data = corpus[random_idx:(random_idx + Tx)]
        for j in range(Tx):
            idx = values_indices[corp_data[j]]
            if j != 0:
                X[i, j, idx] = 1
                Y[i, j-1, idx] = 1
    Y = numpy.swapaxes(Y,0,1)
    Y = Y.tolist()
    return numpy.asarray(X), numpy.asarray(Y), N_values 

def load_music_utils(data):
    print(f"\n=== {load_music_utils.__name__} ===")
    tones_indices, indices_tones = get_corpus_data(data['vocabulary_map'])
    X, Y, N_tones = data_processing(data, tones_indices, 60, 30)
    return (X, Y, N_tones, indices_tones)

# --- Execution ---
if __name__ == "__main__":
    file_path = 'data/original_metheny.mid'
    #data = extract_musical_grammar(file_path)
    data = extract_complex_grammar(file_path)

    if isinstance(data, dict):
        print(f"Total Measures: {len(data['measures'])}")
        print(f"Unique Grammars Found: {data['unique_grammar_count']}")
        print("-" * 30)
        print("Sample Vocabulary (First 15 Unique Tokens):")
        # Print a sample of the complex names found
        for chord_name, token in list(data['vocabulary_map'].items())[:15]:
            print(f"{token} : {chord_name}")
        print("-" * 30)
        print(f"Grammar Sequence Length: {len(data['grammar_sequence'])}")
        print("First 20 tokens of the sequence:")
        print(data['grammar_sequence'][:20])
    else:
        print(data)
    X, Y, N_tones, indices_tones = load_music_utils(data)
    print(f'number of training examples: {X.shape[0]}')
    print(f'Tx (length of sequence): {X.shape[1]}')
    print(f'total # of unique values: {N_tones}')
    print(f'shape of X: {X.shape}')
    print(f'Shape of Y: {Y.shape}')
    print(f"# chords: {len(data['chords'])}, type: {type(data['chords'][0])}")
    print(f"# chord symbols: {len(data['chord_symbols'])}, type: {type(data['chord_symbols'][0])}")

    data = extract_complex_grammar1(file_path)
    if isinstance(data, dict):
        print(f"Total Measures: {len(data['measures'])}")
        print(f"Unique Grammars Found: {data['unique_grammar_count']}")
        print("-" * 30)
        print("Sample Vocabulary (First 15 Unique Tokens):")
        # Print a sample of the complex names found
        for chord_name, token in list(data['vocabulary_map'].items())[:15]:
            print(f"{token} : {chord_name}")
        print("-" * 30)
        print(f"Grammar Sequence Length: {len(data['grammar_sequence'])}")
        print("First 20 tokens of the sequence:")
        print(data['grammar_sequence'][:20])
    else:
        print(data)        
    X, Y, N_tones, indices_tones = load_music_utils(data)
    print(f'number of training examples: {X.shape[0]}')
    print(f'Tx (length of sequence): {X.shape[1]}')
    print(f'total # of unique values: {N_tones}')
    print(f'shape of X: {X.shape}')
    print(f'Shape of Y: {Y.shape}')
    print(f"# chords: {len(data['chords'])}, type: {type(data['chords'][0])}")
    print(f"# chord symbols: {len(data['chord_symbols'])}, type: {type(data['chord_symbols'][0])}")

