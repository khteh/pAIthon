import numpy, copy
from collections import defaultdict, OrderedDict
from itertools import groupby, zip_longest
from music21 import *
# https://github.com/jisungk/deepjazz
#----------------------------HELPER FUNCTIONS----------------------------------#
# https://github.com/cuthbertLab/music21/issues/1813
''' Helper function to parse a MIDI file into its measures and chords '''
def __parse_midi(data_fn):
    """
    A common arrangement of nested Streams is a Score Stream containing one or more Part Streams, each Part Stream in turn containing one or more Measure Streams.
    Parts > Measures > Voice

    You might think that there should be a convenience property .measures to get all the measures. But the problem with that is that measure numbers would be quite different from index numbers. 
    For instance, most pieces (that donâ€™t have pickup measures) begin with measure 1, not zero. 
    Sometimes there are measure discontinuities within a piece (e.g., some people number first and second endings with the same measure number). 
    For that reason, gathering Measures is best accomplished NOT with getElementsByClass(stream.Measure) but instead with either the measures() method (returning a Stream of Parts or Measures) or the measure() method (returning a single Measure).    
    """
    print(f"\n=== {__parse_midi.__name__} ===")
    # Parse the MIDI data for separate melody and accompaniment parts.
    midi_data = converter.parse(data_fn)
    assert len(midi_data.getElementsByClass(stream.Part)) == len(midi_data.parts)
    #print(f"Parts (getElementsByClass): {len(midi_data.getElementsByClass(stream.Part))}")
    #print(f"\nParts (.parts): {len(midi_data.parts)}")
    measure_part5_len = len(midi_data.getElementsByClass(stream.Part)[5].getElementsByClass(stream.Measure))
    measure_part5 = midi_data.parts[5].measures(1, measure_part5_len)
    measure_stream = midi_data.parts[5].getElementsByClass(stream.Measure)
    print(f"\nMeasure (Part[5]): {len(measure_part5)} {measure_part5}")
    #print(f"\nmeasure_part5.show:")
    #measure_part5.show('text')
    #print("recurse():")
    #for el in midi_data.recurse().getElementsByClass(stream.Voice):
    #    print(el.offset, el, el.activeSite)
    # Get melody part, compress into single voice.
    #melody_stream = midi_data[5]     # For Metheny piece, Melody is Part #5.
    print(f"\nMelody Voice: {len(measure_part5.recurse().getElementsByClass(stream.Voice))}")
    melody_voice = []
    for el in measure_part5.recurse().getElementsByClass(stream.Voice):
        #print(el.offset, el, el.activeSite)
        print(f"el offset: {el.offset}, {el}")
        melody_voice.insert(int(el.offset), el)
    #melody1, melody2 =  measure_part5.recurse().getElementsByClass(stream.Voice)
    #for j in melody2:
    #    melody1.insert(j.offset, j)
    #melody_voice = melody1

    for i in melody_voice:
        if i.quarterLength == 0.0:
            i.quarterLength = 0.25

    # Change key signature to adhere to comp_stream (1 sharp, mode = major).
    # Also add Electric Guitar. 
    melody_voice.insert(0, instrument.ElectricGuitar())
    melody_voice.insert(0, key.KeySignature(sharps=1))
    print(f"{len(melody_voice)} melody_voice")

    # The accompaniment parts. Take only the best subset of parts from
    # the original data. Maybe add more parts, hand-add valid instruments.
    # Should at least add a string part (for sparse solos).
    # Verified are good parts: 0, 1, 6, 7 '''
    partIndices = [0, 1, 6, 7]
    accompaniment = stream.Voice()
    #print("\nenumerate(midi_data)")
    #for i, j in enumerate(midi_data):
    #    if i in partIndices and not isinstance(j, metadata.Metadata):
    #        print(f"{i}: {j}, {j}")
    #        j.show("text")

    #comp_stream = measure_stream.voices.stream()
    accompaniment.append([j.flatten() for i, j in enumerate(midi_data) if i in partIndices and not isinstance(j, metadata.Metadata)])

    print(f"\naccompaniment: {len(accompaniment)}")
    #comp_stream.show("text")

    # Full stream containing both the melody and the accompaniment. 
    # All parts are flattened. 
    full_stream = stream.Voice()

    #full_stream = measure_stream.voices.stream()
    for i in range(len(accompaniment)):
        full_stream.append(accompaniment[i])

    full_stream.append(melody_voice)

    print(f"\nfull_stream: {len(full_stream)}")
    #full_stream.show("text")
    solo_stream = stream.Voice()
    for part in full_stream:
        print(f"\npart: {part}")
        #part.show("text")
        curr_part = stream.Part()
        if isinstance(part, stream.Part):
            print(f"\ninstrument.Instrument:")
            for i in part.getElementsByClass(instrument.Piano):
                print(f"instrument: {i}")
                i.show("text")
                curr_part.append(i)
            print(f"\ntempo.MetronomeMark:")
            for i in part.getElementsByClass(tempo.MetronomeMark):
                print(f"MetronomeMark: {i}")
                i.show("text")
                curr_part.append(i)
            print(f"\nkey.KeySignature:")
            for i in part.getElementsByClass(key.KeySignature):
                print(f"KeySignature: {i}")
                i.show("text")
                curr_part.append(i)
            print(f"\nmeter.TimeSignature:")
            for i in part.getElementsByClass(meter.TimeSignature):
                print(f"TimeSignature: {i}")
                i.show("text")
                curr_part.append(i)
            print(f"\nmeter.getElementsByOffset:")
            for i in part.getElementsByOffset(476, 548, includeEndBoundary=True):
                print(f"offset: {i}")
                i.show("text")
                curr_part.append(i)
            cp = curr_part.flatten()
            solo_stream.insert(cp)
    # Extract solo stream, assuming you know the positions ..ByOffset(i, j).
    # Note that for different instruments (with stream.flat), you NEED to use
    # stream.Part(), not stream.Voice().
    # Accompanied solo is in range [478, 548)
    #solo_stream = measure_stream.voices.stream()
    print(f"\nsolo_stream: {len(solo_stream)}")
    solo_stream.show("text")
    # Group by measure so you can classify. 
    # Note that measure 0 is for the time signature, metronome, etc. which have
    # an offset of 0.0.
    melody_stream = solo_stream[-1]
    print(f"\n{len(melody_stream)} melody_stream: {melody_stream}")
    melody_stream.show("text")
    measures = OrderedDict()
    offsetTuples = [(n.offset // 4, n) for n in melody_stream]
    measureNum = 0 # for now, don't use real m. nums (119, 120)
    for key_x, group in groupby(offsetTuples, lambda x: x[0]):
        measures[measureNum] = [n[1] for n in group]
        measureNum += 1
    print(f"\nmeasures: {len(measures)} {measures}")
    # Get the stream of chords.
    # offsetTuples_chords: group chords by measure number.
    chordStream = solo_stream[0] # Only piano has chord
    chordStream.removeByClass(instrument.Piano)
    chordStream.removeByClass(tempo.MetronomeMark)
    chordStream.removeByClass(key.Key)   
    chordStream.removeByClass(meter.TimeSignature)
    chordStream.removeByClass(note.Rest)
    chordStream.removeByClass(note.Note)
    print(f"\n{len(chordStream)} chordStream: {chordStream}")
    chordStream.show("text")

    offsetTuples_chords = [(n.offset // 4, n) for n in chordStream]

    # Generate the chord structure. Use just track 1 (piano) since it is
    # the only instrument that has chords. 
    # Group into 4s, just like before. 
    chords = OrderedDict()
    measureNum = 0
    for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):
        chords[measureNum] = [n[1] for n in group]
        measureNum += 1
    print(f"\nchords: {len(chords)} {chords}")
    # Fix for the below problem.
    #   1) Find out why len(measures) != len(chords).
    #   ANSWER: resolves at end but melody ends 1/16 before last measure so doesn't
    #           actually show up, while the accompaniment's beat 1 right after does.
    #           Actually on second thought: melody/comp start on Ab, and resolve to
    #           the same key (Ab) so could actually just cut out last measure to loop.
    #           Decided: just cut out the last measure. 
    #del chords[len(chords) - 1]
    assert len(chords) == len(measures), f"{len(chords)} chords, {len(measures)} measures"
    print(f"{len(chords)} measures and chords")
    return measures, chords

''' Helper function to determine if a note is a scale tone. '''
def __is_scale_tone(chord, note):
    # Method: generate all scales that have the chord notes th check if note is
    # in names

    # Derive major or minor scales (minor if 'other') based on the quality
    # of the chord.
    scaleType = scale.DorianScale() # i.e. minor pentatonic
    if chord.quality == 'major':
        scaleType = scale.MajorScale()
    # Can change later to deriveAll() for flexibility. If so then use list
    # comprehension of form [x for a in b for x in a].
    scales = scaleType.derive(chord) # use deriveAll() later for flexibility
    allPitches = list(set([pitch for pitch in scales.getPitches()]))
    allNoteNames = [i.name for i in allPitches] # octaves don't matter

    # Get note name. Return true if in the list of note names.
    noteName = note.name
    return (noteName in allNoteNames)

''' Helper function to determine if a note is an approach tone. '''
def __is_approach_tone(chord, note):
    # Method: see if note is +/- 1 a chord tone.

    for chordPitch in chord.pitches:
        stepUp = chordPitch.transpose(1)
        stepDown = chordPitch.transpose(-1)
        if (note.name == stepDown.name or 
            note.name == stepDown.getEnharmonic().name or
            note.name == stepUp.name or
            note.name == stepUp.getEnharmonic().name):
                return True
    return False

''' Helper function to determine if a note is a chord tone. '''
def __is_chord_tone(lastChord, note):
    return (note.name in (p.name for p in lastChord.pitches))

def parse_melody(fullMeasureNotes, fullMeasureChords):
    # Remove extraneous elements.x
    measure = copy.deepcopy(fullMeasureNotes)
    chords = copy.deepcopy(fullMeasureChords)
    measure.removeByNotOfClass([note.Note, note.Rest])
    chords.removeByNotOfClass([chord.Chord])

    # Information for the start of the measure.
    # 1) measureStartTime: the offset for measure's start, e.g. 476.0.
    # 2) measureStartOffset: how long from the measure start to the first element.
    measureStartTime = measure[0].offset - (measure[0].offset % 4)
    measureStartOffset  = measure[0].offset - measureStartTime

    # Iterate over the notes and rests in measure, finding the grammar for each
    # note in the measure and adding an abstract grammatical string for it. 

    fullGrammar = ""
    prevNote = None # Store previous note. Need for interval.
    numNonRests = 0 # Number of non-rest elements. Need for updating prevNote.
    for ix, nr in enumerate(measure):
        # Get the last chord. If no last chord, then (assuming chords is of length
        # >0) shift first chord in chords to the beginning of the measure.
        try: 
            lastChord = [n for n in chords if n.offset <= nr.offset][-1]
        except IndexError:
            chords[0].offset = measureStartTime
            lastChord = [n for n in chords if n.offset <= nr.offset][-1]

        # FIRST, get type of note, e.g. R for Rest, C for Chord, etc.
        # Dealing with solo notes here. If unexpected chord: still call 'C'.
        elementType = ' '
        # R: First, check if it's a rest. Clearly a rest --> only one possibility.
        if isinstance(nr, note.Rest):
            elementType = 'R'
        # C: Next, check to see if note pitch is in the last chord.
        elif nr.name in lastChord.pitchNames or isinstance(nr, chord.Chord):
            elementType = 'C'
        # L: (Complement tone) Skip this for now.
        # S: Check if it's a scale tone.
        elif __is_scale_tone(lastChord, nr):
            elementType = 'S'
        # A: Check if it's an approach tone, i.e. +-1 halfstep chord tone.
        elif __is_approach_tone(lastChord, nr):
            elementType = 'A'
        # X: Otherwise, it's an arbitrary tone. Generate random note.
        else:
            elementType = 'X'

        # SECOND, get the length for each element. e.g. 8th note = R8, but
        # to simplify things you'll use the direct num, e.g. R,0.125
        if (ix == (len(measure)-1)):
            # formula for a in "a - b": start of measure (e.g. 476) + 4
            diff = measureStartTime + 4.0 - nr.offset
        else:
            diff = measure[ix + 1].offset - nr.offset

        # Combine into the note info.
        noteInfo = "%s,%.3f" % (elementType, nr.quarterLength) # back to diff

        # THIRD, get the deltas (max range up, max range down) based on where
        # the previous note was, +- minor 3. Skip rests (don't affect deltas).
        intervalInfo = ""
        if isinstance(nr, note.Note):
            numNonRests += 1
            if numNonRests == 1:
                prevNote = nr
            else:
                noteDist = interval.Interval(noteStart=prevNote, noteEnd=nr)
                noteDistUpper = interval.add([noteDist, "m3"])
                noteDistLower = interval.subtract([noteDist, "m3"])
                intervalInfo = ",<%s,%s>" % (noteDistUpper.directedName, 
                    noteDistLower.directedName)
                # print "Upper, lower: %s, %s" % (noteDistUpper,
                #     noteDistLower)
                # print "Upper, lower dnames: %s, %s" % (
                #     noteDistUpper.directedName,
                #     noteDistLower.directedName)
                # print "The interval: %s" % (intervalInfo)
                prevNote = nr

        # Return. Do lazy evaluation for real-time performance.
        grammarTerm = noteInfo + intervalInfo 
        fullGrammar += (grammarTerm + " ")

    return fullGrammar.rstrip()

def __get_abstract_grammars(measures, chords):
    # extract grammars
    abstract_grammars = []
    for ix in range(1, len(measures)):
        m = stream.Voice()
        for i in measures[ix]:
            m.insert(i.offset, i)
        c = stream.Voice()
        for j in chords[ix]:
            c.insert(j.offset, j)
        parsed = parse_melody(m, c)
        abstract_grammars.append(parsed)
    return abstract_grammars

''' Get musical data from a MIDI file '''
def get_musical_data(data_fn):
    print(f"\n=== {get_musical_data.__name__} ===")
    measures, chords = __parse_midi(data_fn)
    abstract_grammars = __get_abstract_grammars(measures, chords)
    print(f"{len(measures)} measures, {len(chords)} chords, {len(abstract_grammars)} abstract_grammars")
    return chords, abstract_grammars

''' Get corpus data from grammatical data '''
def get_corpus_data(abstract_grammars):
    corpus = [x for sublist in abstract_grammars for x in sublist.split(' ')]
    values = set(corpus)
    val_indices = dict((v, i) for i, v in enumerate(values))
    indices_val = dict((i, v) for i, v in enumerate(values))

    return corpus, values, val_indices, indices_val

def data_processing(corpus, values_indices, m = 60, Tx = 30):
    # cut the corpus into semi-redundant sequences of Tx values
    Tx = Tx 
    N_values = len(set(corpus))
    numpy.random.seed(0)
    X = numpy.zeros((m, Tx, N_values), dtype=numpy.bool)
    Y = numpy.zeros((m, Tx, N_values), dtype=numpy.bool)
    for i in range(m):
#         for t in range(1, Tx):
        random_idx = numpy.random.choice(len(corpus) - Tx)
        corp_data = corpus[random_idx:(random_idx + Tx)]
        for j in range(Tx):
            idx = values_indices[corp_data[j]]
            if j != 0:
                X[i, j, idx] = 1
                Y[i, j-1, idx] = 1
    
    Y = numpy.swapaxes(Y,0,1)
    Y = Y.tolist()
    return numpy.asarray(X), numpy.asarray(Y), N_values 

def load_music_utils(file):
    chords, abstract_grammars = get_musical_data(file)
    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)
    N_tones = len(set(corpus))
    X, Y, N_tones = data_processing(corpus, tones_indices, 60, 30)   
    return (X, Y, N_tones, indices_tones, chords)

if __name__ == "__main__":
    X, Y, n_values, indices_values, chords = load_music_utils('data/original_metheny.mid')
    print('number of training examples:', X.shape[0])
    print('Tx (length of sequence):', X.shape[1])
    print('total # of unique values:', n_values)
    print('shape of X:', X.shape)
    print('Shape of Y:', Y.shape)
    print('Number of chords', len(chords))
    """
    number of training examples: 60
    Tx (length of sequence): 30
    total # of unique values: 90
    shape of X: (60, 30, 90)
    Shape of Y: (30, 60, 90)
    Number of chords 19    
    """