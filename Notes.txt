Conditional Probability:
P(a|b) = P(a ^ b) / P(b)
P(a ^ b) = P(a|b)P(b)
P(a ^ b) = P(b|a)P(a)

P(a|b)P(b) = P(b|a)P(a)
P(a|b) = P(b|a)P(a) / P(b) <= Bayes' Rule
P(b|a) = P(a|b)P(b) / P(a) <= Bayes' Rule

Bayes' Rule: 
Knowing - P(visible effect|unknown cause)
We can conclude - P(unknown cause | visible effect)

Example:
Knowing - P(medical test result | disease)
We can conclude - P(disease | medical test result)

If Probability of 'a' and 'b' are independent,
P(a ^ b) = P(b)P(a)

P(aâˆ£b) = P(bâˆ£a)P(A) / P(B)

Bayes rule/formula:
Prior Odds: 5:95
Likelihood Ratio: P(A|B) / P(A|C) : (80/100) / (10/100) = (80/100) * (100/10) = 80/10 = 8
Posterior Odds: 40:95

Joint Probability:
P(A|b) = P(A, b) / P(b) : Comma signifies ^
       = alpha * P(A, b), where alpha = 1/P(b) which is just some constant.
       = alpha * <Probability distribution of 'b' which sums up to 1>
=> Conditional probability is proportional to Joint probability

Inclusion-Exclusion:
P(a v b) = P(a) + P(b) - P(a^b)

Marginalization: Calculate probability of independent variable based on given/known joint probabilities.
P(a) = P(a,b) + P(a, Not(b))
P(X = xi) = Summation over all the values (j) y could take on of P(X = xi, Y = yj)

Conditioning: Calculate probability of independent variable based on given/known conditional probabilities.
P(a) = P(a|b)P(b) + P(a|Not(b))P(Not(b))
P(X = xi) = Summation over all the values (j) y could take on of P(X = xi|Y = yj)P(Y = yj)

Bayesian Network: Data structure which represents the dependencis among random variables.
- Directed graph
- Each node represents a random variable.
- Arrow from X to Y means X is a parent of Y
- Each node X has a probability distrubution P(X | Parents(X))

Inference:
- Query X: variable for which to compute distribution
- Evidence variables E: observed variables for event e
- Hidden variables Y: non-evidence and non-query variable
- Goal: Calculate P(X | e) where e could be more than 1.

Inference by Enumeration:
P(X | e) = alpha * P(X,e) = alpha * Summation over all the values (j) y could take on of P(X, e, y)
X: query variable
e: evidence
y: ranges over values of hidden variables
alpha: normalizes the result.
Cons: Time complexity grows with complexity of the network

Aproximate Inference:
- Sampling:
  - Rejection Sampling: Filter out samples which do not match event specification
  - Likelihood Weighting:
    - Start by fixing the values of evidence variables.
    - Sample the non-evidence variables using conditional probabilities in the Bayesian Network
    - Weight each sample by its likelihood: the probability of all of the evidence.

Markov Assumption: The assumption that the current state depends on only a finite fixed number of previous states.
Markov Chain: A sequence of random variables where the distribution of each variable follows the Markov assumption.
Hidden Markov Model: a Markov model for a system with hidden states which generate some observed events.
Sensor Markov Assumption: the assumption that the evidence variable depends only on corresponding state.

Supervised Learning > Classification:
Perceptron Learning Rule: Given data point (x,y), update each weight according to: w =  w + alpha*(y - h(X)) * x
Start with random weights, learn from data and update the weights that result in better weight vector which reduces loss.

Support Vector Machines:
- Maximum Margin Separator: Boundary that maximizes the distance between any of the data points
- If the data is not linearly separable, it works from higher dimension to find the separation boundary. For example, other shapes than linear lines.

Optimization Problem Formulation:
- Local Search: 
  - Algorithms: Hill-Climbing, Simulated Annealing
- Linear Programming
  - Algorithms: 
- Constraint Satisfaction
  - Algorithms: AC3, Backtracking

Supervised Learning > Regression:
Learning a function mapping an input point to a continuous value.

Types of loss functions:
0-1 Loss: Used in discrete classification
L1 Loss: |actual - prediction| -> Used in continuous number prediction. Used when we don't care about outliers.
L2 Loss: (actual - prediction)^2 -> Penalizes worse / bigger loss more harshly. Used when we care about outliers.

Underfitting (High bias):
- J(train) AND J(cv) are high
- The model has a strong preconception of how the data  should fit into itself. For example, linear function, etc.
- More data will NOT help.
- Example: Training dataset performance: 64%, Test dataset performance: 47%

Variance = measure of how well the model can generalize to unseen data.

Overfitting (High variance):
- J(train) is low; J(cv) is high
- A model that fits too closely to a particular data set and therefore may fail to generalize to future data. High variance because if the model overfits, a tiny change in one of the training data set will end up completely different model.
- This is a side effect of minimizing loss of a model. If loss = 0, the model may only work on the specific data set.
- One way to counter this problem is add other parameters to optimization. For example, consider complexity:
- More data is likely to help.
- One of the causes is data leakage - some of test data leak into training data.
- Example: Training dataset performance: 93%, Test dataset performance: 99%
Note: Overfitting is sometimes a good thing to begin with. It means the model is learning.

High bias AND high variance:
- J(train) is high; J(cv) higher than J(train)
- Sometimes happen in NN. Less in linear regression.
- Doesn't really happen for linear models applied to 1-D.
- Underfit and overfit on parts of the data at the same time.

Balanced (Goldilocks zone):
- Example: Training dataset performance: 98%, Test dataset performance: 96%

Cost(h) = Loss(h) + w*Complexity(h) : 'w' gives weight to the complexity

Adding the term w*Complexity(h) is called "Regularization": Penalizing hypotheses that are more complex to favour simpler, more general hypotheses
To overcome overfitting:
(1) Collect more training samples / observations.
(2) Use fewer features.
Many features + insufficient data -> overfitting
(3) Regularization - Gently reduces the impact of some of the features without eliminating them outright. It encourages the model to shrink the values of the parameters without necessarily setting them to 0 which eliminates the features.
    Cost(w,b) = (sum((f_w_b(x) - y) ** 2) + lambda * sum(w[j] ** 2)) / 2m
    f_w_b <= Linear regression modal
    sum((f_w_b(x) - y) ** 2) <= Mean squared error. Fit data
    sum(w[j] ** 2) <= Regularization term. Keep w[j] small
    lambda balances both goals
    Example: f_w_b = w1x + w2x^2 + w3x^3 + w4x^4 + b
    lambda = 0: Overfitting. Wiggly curve/graph which tries to fit every single data point in the training data set.
    lambda = 10^10: f_w_b = b Underfitting
Gradient Descent:
w = w - alpha * (dJ(w) / dw)
Gradient Descent withh regularization:
w = w - alpha/m * (sum((f_w_b(x) - y) * x) + lambda * w[j])
  = w * (1 - alpha * lambda / m) - (alpha / m ) * sum((f_w_b(x) - y) * x)
  alpha: 0.01, lambda: 10, m: 100 => (1 - alpha * lambda / m) = (1 - 0.001) = 0.999
  This shows that every iteration of gradient descent, regularization reduces the value of w, thus reduces the impact of the feature.

Strategy to debugging a learning algorithm:
(1) Get more data - Fixes high variance.
(2) Try smaller sets of features (Simplify polynomial equation by removing some of the higher order components.) - Fixes high variance.
(3) Add more features - Fixes high bias
(4) Add polynomial features - Fixes high bias
(5) Decrease lambda - Fixes high bias
(6) Increase lambda - Fixes high variance.

Hold-out cross validation splits data into training and testing data sets. How to split?
k-fold cross-validation: Splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data as training set.

Feature Scaling: When a feature has a large range of values, the weight/parameter calculated by the model will have small range. In this case, a small change in weight will have big change in the cost value.
This causes the gradient descent to take longer time to find it's global minimum, i.e., converge slower. To oversome this, scale the input data. 2 ways:
(1) Divide by the maximum value of the data range. This will produce scaled input data of [0, 1]
(2) Mean normalization: Find the average value (ave). (x - ave) / (Xmax - Xmin). This will produce scaled input data of [-1, 1] which centers around 0.
(3) Z-score normalization: Find the standard deviation (d) and the mean. (x - ave) / d.
    Mean: sum(data) / len(data)
    standard deviation: sum((data[i] - mean) ** 2) / len(data)
Rules of thumb: Aim for [-1, 1] input data range. These need to rescale:
(1) [-100 , 100] : Too large
(2) [-0.001, 0.001] : Too small
(3) [98.6, 105]: Too large. Example, Fahrenheight body temperature.
Notes:
Feature scaling usually isn't required for your target variable (labels).
Feature scaling is usually not required with tree-based models (e.g. Random Forest) since they can handle varying features.
Amount of data used must measure up to the #parameters to fit or train on.

Reinforcement Learning:
Given a set of rewards or penalties, learn what actions to take in the future.
Markov Decision Process:
Model for decision-making, representing states, actions, and their rewards.
Q-Learning:
Method for learning a function Q(s,a), estimate of the value (reward) of performing action 'a' in state 's'. Start with Q(s,a) = 0 for all s,a.
Q(s,a) <- Q(s,a) + w*(new estimate - old estimate): w:0 old values are more important; w:1 new values are more important.
Q(s,a) <- Q(s,a) + w*((r + future reward estimate) - Q(s,a)) 
Q(s,a) <- Q(s,a) + w*((r + w1*max(Q(s',a') for all a')) - Q(s,a)): w1 provides weights for future vs current reward

Balance between Exploration and Exploitation.

Function Approximation:
Approximating Q(s,a), often by a function combining various features, rathen than storing one value for every state-action pair.
Similar to depth-limiting approach in MiniMax. Used when it is not feasible to explore all the posible values of Q(s,a) in a bigger state space.

Unsupervised Learning:
Given input data without any additional feedback (label), learn patterns, find structure in the data.
Tasks:
(1) Clustering: Organize a set of objects into groups in such a way that similar objects tend to be in the same group. Example: Genetic research, Image segmentation, Market research, Medical imaging, Social network analysis.
    One technique is k-means clustering: Algorithm for clustering data based on repeatedly assigning points to clusters (k number of clusters) and updating those clusters' centers
(2) Anomaly Detection: Find unusual events in the data.
    Use case: Fraud detection.
(3) Dimensionality Reduction: Compress large corpuses of data to a much smaller dataset without losing key / important information.

k-means clustering:
(1) Choose a number for 'k' cluster centroids, u[k]. k < len(data)
(2) Randomly assigns 'k's values which have the same dimension as the dataset.
(3) Assign samples to the nearest centroid (k)
    for i of range(len(data)):
      c[i] = index (from 1 to k) of cluster centroids closest to x[i]
    The distance can be calculated with min(|x[i] - u[k]|), or more generally, min(|x[i] - u[k]|  ** 2 )
(4) Move the cluster centroids to the average of the points
    for i of range(k):
      u[i] = average (mean) of points assigned to cluster k
    The means can be calculated with sum(x[]) / len(x[])
There will be situations where one of the uk has zero data points assigned to it. In this case, either
(1) Remove the empty cluster: k = k - 1
(2) Reinitialize the cluster centroids and restart the algorithm.
Optimization objective:
c[i] = index of cluster (1,2,...,k) to which sample x[i] is currently assigned to.
uk = cluster centroid k
uc[i] = location of centroid k to which x[i] is currently assigned to.
cost function: J(c, uk) = sum(|x[i] - u[k]|  ** 2 ) / m
Every step along the way reduces J(c, uk).
To minimize local minimum and choose the best cluster centroids, use multiple random initializations and choose the cluster with the lowest J(c,u). i.e., run the algorithm multiple times, maybe 50 to 1000.

Anomaly Detection:
Density Estimation: Model p(x) from data. Probability of x[i] being seen in the dataset.
p(x[i]) < epsilon : Anomaly
p(x[i]) >= epsilon: OK
p = Gaussian distribution - Needs mean and sigma (standard distribution); sigma ** 2 is variance
To model p(x) with multiple featues (N), every feature is a Gaussian distribution.
(1) Calculate the mean and sigma of every feature.
(2) p(x) = p(x1; u1,s1) * p(x2; u2,s2) * p(x3; u3,s3) * ... * p(xN; uN,sN)

Anomaly Detection vs Supervised Learning:
Anomaly Detection is mostly used when:
(1) Very small number of positive exaples (y=1). 0-20 is common. Large number of negative (y=0)
(2) Many different "types" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous samples seen so far (training set)
Examples: Security related applications like fraud, defects detection of manufacturing very complex products like the aircraft engines, monitoring complex systems like in a data center.

Supervised Learning:
(1) Large number of positive and negative examples
(2) Enough positive examples for algorithm to get a sense of what positive examples are lke; future positive samples likely to be similar to ones in training set.
Examples: Email spam classification, defects detection of mobile phone manufacturing, weather prediction, disease classification.

Anomaly Detection feature selection:
(1) Choose feature with gaussian distribution property (plt.hist(x)). Otherwise, try to transform it:
    (i) log(x + C) : Lower values of C transform better
    (ii) sqrt(x)
    (iii) x ^ 1/3

Anomaly Detection error analysis:
Most common problem: p(x) is comparable for both normal and anomalous samples. p(x) is large. In this case, the algorithm will fail to flag an anomalous test data. Solutions:
(1) Check the anomalous test data. Create new feature which helps detect the anomaly.
(2) Create new feature based on existing ones using simple equations like x1 / x2 or (x1^2) / x2, etc.

Principal Component Analysis:
(1) Commonly used for visualization. Example: Take data with many features, say 50, 1000 or even more, reduce the #features to a few so as to enable visualization / plotting.
    - Common used by data scientists to visualize data for analysis.
    - Choose feature(s) with meaningful degree of variation.
    - Create new feature(s) based on existing ones. Example: length x height
(2) Features should first be normalized to have zero mean before applying PCA.
(3) Features should first be scaled before applying PCA.
(4) It will choose an axis(the Principal Component) around the origin which results in the largest variance when projecting the data onto it (at 90 degree angle).
    - All new axes created will be perpendicular to one another (90-degree)
(5) Less common applications:
    (i) Data compression - Reduce storage and/or transmission cost
    (ii) Speed up training of a supervised learning model.
=========================================================================================================================================================================================================================================
Neural Network (multilayer perceptron):
- Scale drives DL progess.
  (1) Data - especially labelled data (X,Y).
  (2) Computation - Speeds up Idea -> Code -> Experiment iterations.
  (3) Algorithms - Algorithmic innovation which makes computaion runs faster. For example, switching from Sigmoid to ReLU enables graduent descent to run faster by eliminating the long tails of Sigmoid.
Non-Linear Activation functions:
(1) Step function: g(x) = 1 if x >=0, else 0
(2) Logistic Sigmoid: 
    - g(x) = e^x / (1 + e^x)
    - derivative is minuscule when z is both -ve/+ve large numbers. Not good for gradient descent.
    - Never use this in the hidden layers. Only in the output layer for binary classification.
    - Derivative = a * (1 - a)
(3) tanh:
    - (e^z - e^(-z)) / (e^z + e^(-z))
    - Similar to sigmoid graph but the output is [-1, 1]. 
    - Center at (0,0) means easier for the NN to learn.
    - derivative is minuscule when z is both -ve/+ve large numbers. Not good for gradient descent.
    - Derivative = 1 - a ** 2
(4) Rectified Linear Unit (ReLu):
    - g(x) = max(0,x)
    - derivative = 0 for z <= 0, 1 otherwise.
    - Preferred default choice for hidden layers.
    - Derivative = 1 if z > 0; 0 otherwise
(5) Leaky Rectified Linear Unit (Leaky ReLu):
    - g(x) = max(0.01 * z, x)
    - derivative = 0.01 for z <= 0, 1 otherwise.

Choices of Activation Function:
(1) Output layer: Depends on the nature of the label:
    (i) Binary classification: Sigmoid
    (ii) -/+: Linear. For example, stock price change 
    (iii) Only positive numbers: ReLU
(2) Hidden layers: Mostly use LeLU. Reasons:
    (i) Simpler calculation compared to Sigmoid. This makes NN runs faster.
    (ii) There are 2 horizontal asymtotes in Sigmoid. This results in derivatives being close to 0 and therefore make gradient descent slower to run to global minimum.
    Do NOT use linear/identity activation function, g(z) = z in hidden layers. This will result in a linear function and defeats the purpose of NN. The computaion of multiple linear funtions is itself a linear function.
    The "off" or disable feature of the ReLU activation enables models to stitch together linear segments to model complex non-linear functions.
Types of Layers:
(1) Dense: Every neuron in a layer computes it's output / activation using ALL inputs/activations from ALL neurons in previous layer.
(2) Convolution: Neurons only look at regions of the data. Benefits:
    (i) Faster computation.
    (ii) Need less training data - less prone to overfitting.

Random Weights Initialization:
- If initialize to zeros, all the hidden units will be computing the same function. dW will have the same rows after every epoch and therefore same as W.
- b is ok to initialize to zeros.
- W = numpy.random.randn((m,n)) * 0.01
  - * 0.01 so as NOT to generate big numbers which will make NN slow to learn when using sigmoid or tanh activation functions.
  
h(x1,x2) = g(w0 + w1x1 + w2x2 + ...)
Example:
(1) To model the OR function:
    h(x1,x2) = g(-1 + x1 + x2); w0 = -1, w1=w2=1
(2) To model the AND function:
    h(x1,x2) = g(-2 + x1 + x2); w0 = -2, w1=w2=1
Training - to calculate the parameters:
- Trade-offs between speed and accuracy
(1) Gradient Descent: Algorithm for minimizing loss when training NN
    Start with a random choice of weights, repeat:
    - Calculate the gradient based on ALL data points (training data set): direction that will lead to decreasing loss
    - Update weights according to the gradient
(2) Stochastic Gradient Descent: Same as Gradient Descent but "... based on ONE data point"
(3) Mini-Batch Gradient Descent: Same as Gradient Descent but "... based on ONE SMALL BATCH of data points"
Perceptron: Only capable of learning linearly separable decision boundary
Multilayer NN: Artificial NN with an input layer, an output layer, and at least one hidden layer. Capable of modelling more complex problems compared to perceptron.
To train multi-layer NN, we have to propagate the loss/error from the output back to the hidden layers:
Back Propagation: Algorithm for training NN with hidden layers
- Start with a random choice of weights, repeat:
  - Calculate error for output layer
  - For each layer, starting with output layer, and moving inwards towards the earliest hidden layer
    - Propagate error back one layer
- One step of back propagation will provide a derivative of the output variable with respect to the input variable at that layer.
- Using calculus chain rule, i.e., dJ/da == dJ/dv * dv/da, Backpropagation helps calculate derivative of final output, J, with respect to the input, a.

To prevent underfitting (high bias):
(1) Use a larger network.

To prevent Overfitting (high variance):
(1) Collect more data
(2) Choose lambda appropriately.
Dropout: Temporarily removing units - selected at random - from a NN to prevent over-reliance on certain units
playground.tensorflow.org
Image Convolution: Applying a filter that adds each pixel value of an image to it's neighbours, weighted according to a kernel matrix.
                  - Extracts features from input. Output is a feature map
Pooling: reducing the size of an input by sampling from regions in the input
(1) Max-pooling: pooling by choosing the maximum value in each region
Convolutional NN: NN that uses convolution, usually for analyzing images (i.e., Image Convolution + Pooling in a NN)
                - Training is done to figure out what's the best filters for the input image
                - Models how human look at images
Input image -> [Convolution (calculates filters) -> Pooling (Summarize and reduces the size of inputs)] -> Flattening (Feeds into the inputs of NN)
[Convolution -> Pooling] can be applied multiple times before Flattening. Early layers of this to discover low-level features (Edges, Curves, Shapes); later layers to discover high-level features (objects, person, face, etc)
Note: In a convnet, the higher up a layer is, the more specialized it is. The first few layers in a convnet learn very simple and generic features, which generalize to almost all types of images. But as you go higher up, the features are increasingly specific to the dataset that the model is trained on.

Feed-forward NN: Good for classification. 1:1 mapping from input to output -Single-valued output.
Recurrent NN: Output from the NN is fed to it's input for future calculation. It maintains some states. Good for dealing with sequences of data, both input and/or output. N:N mapping from input to output - Output sequence of values.
              Example applications: Youtube to analyze videos, Google translator, etc
              One example is Long Short Memory Network
=========================================================================================================================================================================================================================================
File formats for training and evaluation.
For large data sets, load the data from data warehouse into SSD storage (Cloud or local). This will prevent underusage of resources, GPU/CPU during the traning/evaluation process.
JSONL: JSON Lines is a simple text-based format with rows. It is human readable and an ideal choice for small to medium-sized datasets.
TFRecord: Binary format and easier to read for computers, ideal for efficient training.
Parquet: Good for large and complex datasets.
=========================================================================================================================================================================================================================================
Data Augmentation:
(1) Image: replicate and distort/transform original data sets
(2) Voice: Add noisy background, audio on bad connection, etc.

Data Synthesis:
(1) Image OCR

Transfer Learning:
Example: A NN trained on 1 million images of diverse object types (animal, fruits, plants, people, etc). You want to come up with a NN for character OCR.
Make a copy of the NN, replace the output layer with a 10-unit layer. Then, there are 2 options to train the "new" NN:
(1) Fine Tuning. Only train output layer parameters - This works if having a small data set.
(2) Train all parameters - This works with a huge data set. The initial phase of training is called pre-training.
The NN trained on 1 million images is called "supervised pretrained model".
The "new" copy of the NN is called "fine-tuning".
This works because layers of neurons have learnt different parts of the images. This could generalize to other types of images not found in the original dataset.
Only use this when:
(1) The same input types: text, images, audio.
(2) Original task has a lot more data than the new task.
(3) Low level features from original task could be helpful to learn the new task.
Use-case exmaples: GPT-3, BERT, imageNet, Speech-recognition -> wake/trigger word detection, Image classification to radiology classification.

Multi-task Learning:
- One NN do several things at the same time. Every task helps, hopefully, all of the other tasks.
- Same NN when all the tasks share similar or same low-level features / early layers of the NN. The output layer will be y^ with shape (n,m). n: #tasks, m: #samples.
  - Loss function = sum(sum(L(y^(j)(i), y(j)(i)) for j:[i:n] )) for i: [1:m]
  -  L is the usual logistic loss: -y(j)(i) * log(y^(j)(i)) - (1 - y(j)(i)) * log(y^(j)(i))
  - Benefit: Works well for partially labelled data. The loss function sum over j with valid value of [0,1] and omit inf, -inf, or NaN.
Only use this when:
- Tasks having similar low-level features.
- Amount of data for each task is quite similar. The aggregate of N-1 tasks' data should be a lot more than the remaining 1 other task. Symmetrically, data of N-1 tasks augment training of every other tasks.
- Can train a big enough NN to do well on ALL of the tasks. This NN should perform better than having a separate NN for each of the tasks.

End-to-End Deep Learning:
- A NN that replaces a traditional multi-staged learning pipeline and removes intermediate steps.
- Only use this when there is large engough end-to-end dataset (X,Y).
- Carefully choose what types of X->Y mappings to learn depeneding on what task the data could be obtained for.
Pros:
(1) Let the data speak for itself.
(2) Reduce hand-crafted / manual components. Dont' force the ML to learn the archaic ways.
Cons:
(1) Excludes potentially useful hand-crafted / manual components. This is especially true when there is lack of end-to-end data. The NN don't have sufficient data to learn the features to map X to Y in this case.
(2) Sometimes divide-and-conquer works better. Need to weight the relative complexity vs data availability.

Skewed Datasets:
Precision = #True positives / (#True + #False positives) = #True positives / (#Total predicted positives)
Recall = #True positives / (#True positives + #False negatives) = #True positives / (#Total actual positives) - Helps detect if the learning algorithm is predicting negatives all the time because the value will be zero.
0 Precision and/or Recall is bad.
For use cases with skewed classes or a rare class, decently high precision and recall values helps reassures the usefulness of the learning algorithm.

Trade-off between Precision and Recall:
Logistic regression: 0 <= F(w,b) <= 1
Predict 1 if F(w,b) >= threshold
Predict 0 if F(w,b) < threshold
To predict 1 only if very confident, use high value of threshold. This results in high precision, low recall
To predict 1 even when in doubt, use low value of threshold. This results in low precision, high recall

Picking the threshold is not something can be done with cross-validation. It's up to the business needs / use-case.
To automatically decide on the best learning algorithm without having to manually select between Precision and Recall, choose the algorithm with the highest F1 score.
F1 score = 2 * PR / (P + R) <= Harmonic mean. A mean calculation which pays attention to the lower value.

Classification model evaluation metrics:
(1) Accuracy
(2) Area under ROC (Receiver Operating Characteristics) curve - binary classification models.
    - Comparison of a model's true-positive rate to false-positive rate at different classification thresholds.
    - The AUC metric tells you how well your model is at choosing between classes (for example, how well it is at deciding whether someone has heart disease or not). A perfect model will get an AUC score of 1.
(3) Confusion matrix
(4) Classification Report

Regression model evaluation metrics:
(1) R^2
(2) Mean absolute error (MAE)
(3) Mean squared error (MSE)

Tune model hyperparameters on Evaluation dataset.
3 ways to adjust hyperparameters:
(1) Manually
(2) Randomly with RandomSearchCV
(3) Exhaustively with GridSearchCV - Brute force, trying every single combination.

2 ways to save and load models:
(1) With Python pickle module.
(2) With joblib module
=========================================================================================================================================================================================================================================
Decision Tree Learning:
Decision 1: How to choose what feature to split on at each node?
            Goal: Maximize purity / Minimize impurity - decision which gives fastest path to leave nodes.
Decision 2: When do you stop splitting?
            (1) When a node is 100% one class
            (2) Tree depth. Deep tree could result in overfitting / high variance.
            (3) When improvement on purity score are below a threshold.
            (4) When the #samples in a node is below a threshold.
Entropy as a measure of impurity
p1 = Fraction of samples that are positive
p0 = 1 - p1
H(p1) = -p1log2(p1) - (1 - p1)log2(1 - p1)
      = -p1log2(p1) - p0log2(p0)
Note:
(1) Use log2 to make the symmetric bell curve range from 0 to 1
(2) 0log2(0) = 0 <- log(0) is negative infinity

Decision 1: Goal: Reduce entropy, H(p1) = Information gain.
Information Gain = H(p1root) - (wl * H(p1l) + wr * H(p1r))
wl = weight of left branch = #left samples / #root samples
wr = weight of right branch = #right samples / #root samples

Regression Tree: Decision Tree which predicts a number.
- Use variance of the sample labels to decide which features to split on.
- Split on feature which gives highest reduction in variance (highest information gain)
Information Gain = V(p1root) - (wl * V(p1l) + wr * V(p1r))

Any single decision tree is highly sensitive to small change in the data. Solution is to use tree ensemble to let multiple trees vote on the final prediction.
(1) Use sampling with replacement to build data for tree ensemble. This is called "bagged" decision tree (https://medium.com/data-science/understanding-sampling-with-and-without-replacement-python-7aff8f47ebe4)
(2) Random forest - Randomizing the feature selection. Select from k < n features at each node. k is usually sqrt(n). Works better for large feature set.
(3) XGBoost - Instead of picking from all examples with equal 1/m probability, make it more likely to pick misclassified samples from all the trees constructed so far.

Decision Trees vs. Neural Networks
Decision Trees and Tree Ensembles:
(1) Works well on structured data
(2) Not recommended for Unstrcutured data (video, audio, images, text)
(3) Fast to train
(4) Small decision trees maybe human interpretable.

NN:
(1) Works well on ALL types of data and the combination of them.
(2) Maybe slower to train compared to decision trees
(3) Works with transfer learning - Can use pretrained models to train the output layer with fewer data.
(4) Can string together multiple NN when building a system of multiple models working together.
=========================================================================================================================================================================================================================================
Collabortive Filtering: Recommend items to user based on ratings of other users who gave similar ratings as the user.
Content-based Filtering: Recommend items to user based on features of both the user and the items to find a good match.
  - R[i,j]=1 if user[j] has rated item[i]
  - Y[i,j] ratings of user[j] on item[i]
  User and item will have different sizes of features. Prediction can be simplified to numpy.dot(W[j], X[i]). bias doesn't affect the performance of the model much.
  For the calculation to work, need to transform the feature vectors to vectors of same dimension. Use NN to achieve this. Call it User NN and Item NN with the output layers having the same #neurons.
  These 2 NNs are trained as a single NN with ONE cost function J = sum((V[u,j] * V[m,i] - Y[i,j]) ** 2) + regularization for all R[i,j] = 1 (Use tf.keras.layers.Dot layer)
Recommendation from LARGE Catalogue - can be done with 2 steps: Retrieval and Ranking.
Retrieval:
(1) Generate last list of plausible item candidates:
    - For each item purchased by the user, find 10 most similar items. This can use overnight batch to calculate the min(|V[k] - V[i]| ** 2)
    - For most viewed 3 categories/genres, find top 10 items.
    - Top 20 items in the country/region.
(2) Combine the retrieved items into a list, removing duplicates and items already purchased.
Ranking:
(1) Use the NN to calculate predictions on the retrieved list. Only need to calculate the missing V[j] or V[i] for the final dot product. Can be done on separated NN. This should be quick.
(2) Display ranked items to the user.
To speed up the response times of the system, pre-compute vectors V[i] for ALL items that it might recommend. This can be done even before a user logs in or even before Xu or Vu vector is computed.
If 2 items have vectors V[i] and V[k] which are similar to one another, i.e., |V[k] - V[i]| is small, a conclusion could be drawn that they will be liked by similar users.
=========================================================================================================================================================================================================================================
Reinforement Learning
State action value function (Q-function)
Q(s,a) = Return if it, the agent:
(1) Start at state s
(2) Take action a (ONCE)
(3) Behave optimally after that: max(Q(s', a'))
The best possible return from state s is max(Q(s,a))
The best possible action in state s is the action which  gives max(Q(s,a))

Policy, pi(s) = a It's job is to take as input any state, s, and map it to some action that it wants the agent to take in that state.
Goal of Reinforcement learning: Find a policy, pi, which tells the agent what action (a = pi(s)) to take in every state (s) so as to maximize the return.
Bellman Equation: Q(s,a) = R(s) + gamma * max(Q(s', a'))
gamma is discount factor [0, 1]: smaller value means higher discount and makes the agent impatient, larger value makes it more patient to obtain reward at later steps. Usually close to 1. Example, 0.9, 0.99, etc.

In random/Stochastic environment, there will be multiple sequence of different rewards due to uncontrolled environment / misstep probabilities. A Policy execution could produce many different results.
In this case, choose a Policy (pi) which maximizes the average/expected sums of discounted rewards.
Bellman Equation: Q(s,a) = R(s) + gamma * E(max(Q(s', a'))); E = Average

In continuous state-space, s is a vector of numbers (some continuous numbers, some binary). Encode the actions (a) using one-hot feature vector.
Use Deep Reinforcement Learning to compute Q(s,a) at the current state (s). X = [s a]. In a state, s, use the NN to compute Q(s, a) of all possible actions to take. Then, pick the action which maximizes Q(s,a).
Use Bellman Equation to generate training datasets with Q(s,a) = X; R(s) + gamma * max(Q(s',a')) = Y

Learning Algorithm:
Input: [s, a]; Output: Q(s,a)
Initialize NN randomly as guess of Q(s,a).
Repeat {
  Take actions wuth the agent. Get (s,a,R(s),s') tuples. <= **
  Store 10,000 most recent tuples. (Replay Buffer)

  Train NN:
    Create training set of 10,000 samples using x = (s,a); y = R(s) + gamma * max(Q(s', a'))
    Train Qnew such that Qnew(s,a) ~= y (f_w_b(x) ~= y)
  Set Q = Qnew ***
}
Q will improve after every training. This is Deep Q-Learning/Q-Network and can be trained by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation.

To be more efficient, X = [s] and Y = Q(s, a) of all possible actions. Output layer has the number of neurons equal to #actions. With this a single inference will generate all the possible Q(s,a) with all possible actions. Then, the max(Q(s',a')) term is readily available.

Using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. Here is why:
Bellman equation error: Y - Q(s,a;w) = R(s) + gamma * max(Q(s', a'; w)) - Q(s,a; w). 
Notice that Y will keep changing in every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create a separate neural network for generating the Y targets. We call this separate neural network the target Q-Network and it will have the same architecture as the original Q.
Steps:
Every C time steps we will use the Q-target-Network to generate the Y targets and update the weights of the Q-target-Network using the weights of the ð‘„-Network. We will update the weights of the the Q-target-Network using a soft update.

To avoid instabilities, use a Target Network and Experience Replay.

** How to choose actions while still learning? 2 options:
(1) Pick the action a which maximizes Q(s,a) - Because of random initialization, the NN might get stuck in it's own misconception that some actions (a) are *always* bad and therefore, would never get a chance to learn them well.
(2) epsilon-greedy policy (e = 0.05)
    (i) Greedy/Exploitation: With probabiity 0.95, Pick the action a which maximizes Q(s,a)
    (ii) Exploration: With probability 0.05, pick an action randomly.
    Start e high (1.0) and gradually decreases it to 0.01 in the course of the training.

Mini-batch helps in both Supervised and Reinforcement Learning when datasets is large and it takes every iteration to compute sum((f_w_b(X) - y) ** 2)/2m over large datasets and therefore takes very long time to converge and computationally expansive. However, mini-batch will exhibit some unreliable behaviour ("noisy") in graident descent although eventually it will converge faster compared to usual batch learning.
*** To make the NN converge more reliably, soft-update learning will help:
(1) Prevent abrupt changes to the NN
(2) Prevent one worse learning step overwrite the existing better NN
Q(w,b) = Qnew(w_new, b_new)
W = 0.01 * Wnew + 0.99 * W
B = 0.01 * Bnew + 0.99 * B
0.01 and 0.99 adds up to 1 and are the hyperparameters which control how aggressively or gradually the training updates the NN parameters.

In machine learning, logits are the raw, unnormalized scores or outputs from a neural network's final layer before an activation function like softmax or sigmoid is applied.
Binary Classification uses Sigmoid activation function and BinaryCrossEntropy loss function.
Multi-class Classification uses Softmax activation function and CategoricalCrossEntropy loss function.

Input: 128x128 greyscale image
Dense: 256 neurons
#parameters = 128 * 128 * 256 + 256 = 4194560 parameters
================================================================================================================================================================================
Convolution:
Early/Shallow layers often see the finer details with simpler shapes and patterns in smaller regions of the input (low-level features) such as edges and simple textures.
Later/Deeper layers often see more complex shapes and patterns in larger regions of the input (high-level features) such as more complex textures and object classes.
In other words, early/shallow layers zoom in, later/deeper layers zoom out.

Vertical edge detection filter: [[1, 0, -1],
                                 [1, 0, -1],
                                 [1, 0, -1]]
Horizontal edge detection filter: [[1, 1, 1],
                                   [0, 0, 0],
                                   [-1, -1, -1]]
Sobel (vertical) filter: [[1, 0, -1],
                          [2, 0, -2],
                          [1, 0, -1]]
Schorr (vertical) filter: [[3,  0, -3],
                           [10, 0, -10],
                           [3,  0, -3]]
NN to learn the filter parameters:  [[w1, w2, w3],
                                     [w4, w5, w6],
                                     [w7, w8, w9]] to detect various types of edges at various orientations.

To prevent data source shrinking, use padding.
"Valid" convolution: 
- No padding.
- nxn * fxf => n-f+1 * n-f+1

"Same" convolution: 
- Use padding, p, so that the output size is the same as the input size.
- n+2p-f+1 * n+2p-f+1
- for output size equals to n, n+2p-f+1 = n => p = (f-1)/2

By convention, in computer vision, f is always odd number. Reasons being:
(1) It results in p being a whole number. This gives symmetric padding around the data source.
(2) It produces a central cell in the filter which facilitates identifiying the position of the filter.

Strided convolution:
- Instead of moving the filter by 1 position, move by the number of strides, s.
- Output size: floor(((n + 2p) / s) + 1) * floor(((n + 2p) / s) + 1). Use of floor denotes that the convolution is only carried out of the filter is completely inside the image including the padding, if required. None of the filter should be hanging outside of the image.

Convolution over volume:
For example, for images with 3 channels, RGB, the filter must also have the same number of channels. The output will NOT have any channel, i.e., 1 dimension less compared to the input.

Multiple filters:
Each filter works on different features and produces one output. All output will be stacked to form the final output.

f[l] = filter size
p[l] = padding
s[l] = stride
Nc[l] = filter#
Each filter: f[l] x f[l] x Nc[l-1]
Activations: a[l] -> Nh[l] x Nw[l] x Nc[l]
Weights: f[l] x f[l] x Nc[l-1] x Nc[l]
bias: Nc[l]; The bias vector is b, where each filter has its own (single) bias.

Input:  Nh[l-1] x Nw[l-1] x Nc[l-1]
Output: Nh[l] x Nw[l] x Nc[l]
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1
Nc[l] = Filter#

One property of convolution network is that the number of parameters are independent of the size of the input. This reduces overfitting.

Example parameters# calculation:
Input: 256x256 RGB
First hidden layer: 64 neurons.
NO Convolution
Bias: Nc[l] = 64
This hidden layer has 256 * 256 * 3 * 64 + 64 = 12582976 parameters

Input: 256x256 grayscale image
Filter: 128 3x3
  Each filter: 3x3x1 = 9
Activations:
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(256 - 3) + 1 = 254
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(256 - 3) + 1 = 254
254x254 = 64516
Weights: 3x3x1x128 = 1152
Bias: Nc[l] = 128
This hidden layer has 1152 + 128 = 1280 parameters

Input: 256x256 RGB
Filter: 128 7x7
  Each filter: 7x7x3 = 147
Activations:
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(256 - 7) + 1 = 250
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(256 - 7) + 1 = 250
250*250*128 = 8000000
Weights: 7x7x3x 128 = 18816
Bias: Nc[l] = 128
This hidden layer has 18816 + 128 = 18944 parameters

Example output volume calculation:
Input: 121x121x16
Filter: 32 4x4 s:3 p:0
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(121 - 4)/3 + 1 = 40
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(121 - 4)/3 + 1 = 40
Output volume = Nh[l] x Nw[l] x Nc[l] = 40 * 40 * 32 = 51200

Input: 63x63x16
Filter: 32 7x7 s:2 p:0
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(63 - 7)/2 + 1 = 29
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(63 - 7)/2 + 1 = 29
Output volume = Nh[l] x Nw[l] x Nc[l] = 29 * 29 * 32 = 26912

Input: 128x128x12
Max Pooling: filter size:4, stride: 4
Nh[l] = floor((n+2p-f) / s) + 1 = 32
Nw[l] = floor((n+2p-f) / s) + 1 = 32
Output volume = Nh[l] x Nw[l] x Nc[l] = 32 x 32 x 12

Input: 66x66x21
Max Pooling: filter size:3, stride: 3
Nh[l] = floor((n+2p-f) / s) + 1 = 32
Nw[l] = floor((n+2p-f) / s) + 1 = 32
Output volume = Nh[l] x Nw[l] x Nc[l] = 22 x 22 x 21

Input: 3D data with 64x64x64x3
Filter: 4x4x4
#filters = 16
Padding: 0
Stride: 2
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(64 - 4)/2 + 1 = 31
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(64 - 4)/2 + 1 = 31
Nd[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(64 - 4)/2 + 1 = 31 
Output volume = Nh[l] x Nw[l] x Nd[l] x Nc[l] = 31 x 31 x 31 x 16

Typically in Convolutional NN, the size of the data will reduce deeper into the NN while the filter# will increase.

Types of layers in a Convolutional NN:
(1) Convolution
    - Relatively few parameters.
(2) Pooling
    - Reduce the size of the representatio
    - Speed up computation.
    - Makes some of the features that it detects more robust.
(3) Fully Connected (Dense)

Pooling:
- Preserve detected features.
(1) Max or Average.
(2) Hyperparameters: f and s
(3) No parameters to learn. Gradient descent does not learn change anything of it in this layer.
(4) Padding. Usually not used with Max pooling.
(5) Reduces/Shrinks Nh[l] and Nw[l]

1x1 convolution:
- To shrink channel#, use 1x1 filter. Example: (28,28,192) -> (28,28,32) can be achieved by a convolution layer of 32 x (1,1,192) filters with ReLU activation. This 1x1 conv filter is also called the bottleneck layer. 
- When implemented within reason, this bottleneck layer helps reduce computational costs significantly by shrinking down the representation size significantly without affecting the performance of the NN.
- One of the key ideas in Inception Network.

Benefits of Convolutional layers:
(1) Parameter sharing
    - Same parameters of the filter are applied in different parts of the input dataset.
(2) Sparsity of connection
    - Each output value depends only on a small number of inputs as determined by the size of the filter.
These benefits result in:
(1) Allow convolutional NN to be trained with smaller training datasets and less susceptible to overfitting.
(2) Good at capturing translation invariance. For example, input dataset shifted a few pixels but the same features are detected.

Residual Network:
- In a plain NN, as the networks gets deeper, the optimization algorithm (gradient descent, etc), has a much harder time in training. The training gets worse when the network gets deeper.
- ResNet helps mitigate this vanishing and exploding gradient problem.
- Identity function is easy for the residual block to learn. For example, adding a residual block at the end of a large NN with output a[l],
  a[l+2] = g(z[l+2] + a[l]) = g(W[l+2]*a[l+1] + b[l+2] + a[l])   a[l] is the residual connection.
  When L2 regularization is applied, W[l+2] is shrunk. For the sake of argument, when W[l+2] and b[l+2] is almost zero, and with ReLU activation, a[l+2] reduces to g(a[l]) = a[l]
- So, adding a residual block does not only hurt performance, it helps the network to learn at least the identity function. Sometimes, this block learns something useful.
- For the addition of the residual connection to work, "Same" convolution is used or apply a weight matrix, Ws to the residual connection. Ws could be weights that are learnt or a fix matrix that implements zero paddings that takes a[l] and zero pads it to the same dimension as z[l+2].
- Very deep "plain" networks don't work in practice because vanishing gradients make them hard to train.
- Skip connections help address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function.
- There are two main types of blocks: The identity block and the convolutional block.
- Very deep Residual Networks are built by stacking these blocks together.    

Inception Network:
- Consists of the same inception module repeated along the network.
- Name originates from Hollywood movie Inception. Meme: https://knowyourmeme.com/memes/we-need-to-go-deeper

Computaional Cost = #filter params x #filter positions x #filters
Example: 
Input: 6x6x3 (RGB)
Filter: 3x3x3 (Nc has to match)
Stride: 1
Activations:
Nh[l] = floor((Nh[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(6 - 3) + 1 = 4
Nw[l] = floor((Nw[l-1] + 2p[l] - f[l]) / s[l]) + 1 = floor(6 - 3) + 1 = 4
1 filter: (3 * 3 * 3) * (4 * 4)
5 filters: (3 * 3 * 3) * (4 * 4) * 5 = 2160

Depthwise separable convolution reduces the computaional costs by 2 steps:
(1) Depthwise convolution.
    - Apply a single filter to the input. This results in a Nh x Nw x 1 output.
(2) Pointwise convolution.
    - Apply 1x1xNc' filter to the output from (1). This produces the final Nh x Nw x Nc' output.
- It reduces the computaional costs to (1/Nc' + 1/f^2) of the conventional convolution. For example, Nc'=512, f=3 will result in ~10 times cheaper.
- This is the core of MobileNet.
MobileNet v2:
- Bottleneck layer consists of 3 convolution layers and 1 residual connection (ResNet):
(1) Expansion - Use 1x1xNc' to expand the dimension
(2) Depthwise convolution
(3) Pointwise convolution - Projects back to smaller representation.
- (1) and (2) allows the NN to learn richer functions.
- (3) helps reduces memory footprint.

MobileNetv2 Bottleneck block. Input: nxnx5, 30 expansion filters, 20 projection filters. How many parameters in the complete block?
Expansion: (1x1x5) * 30 = 150
Depthwise: nxnx30 -> 3x3x30 = 270
Pointwise: (1x1x30) * 20 = 600
Total: 1020 

EfficientNet:
- 3 ways to scale up or down depending on the device computational resource constraints:
(1) Resolution of the input. r
(2) Depth of the NN. d
(3) Make the layers wider. w

Transfer Learning:
- Usually used in computer vision applications.
(1) Freeze all layers except the final softmax activation to cater to different classification.
    - Precompute the layers activation using custom inputs and save the feature vectors for prediction.
    - Use this if having small dataset.
(2) Freeze fewer early layers, train later layers. Or replace the last few layers.
    - Use this if having larger dataset.
(3) Retrain the whole NN if having large dataset.

Object Detection:
Sliding Windows Detection Algorithm - Computationally expansive due to sequential calculation of the parameters.
Convolution implementation of the sliding windows - One single forward pass through the NN computes all the windows due to the property of convolution which is parameter sharing.
- It works by turning the last Dense / fully-connected layers into convolution layers.
Intersection over Union
- Used to evaluate the accuracy of object localization.
- size(intersection box) / size(union box)
- "Correct" if >= threshold (e.g: 0.5)
Non-max suppression algorithm:
- Discard all boxes with Pc <= 0.6
- For each of the remaining boxes,
  - Pick the box with the higest Pc as the prediction.
  - Discard any box with IoU >= 0.5 with the prediction box. This discards the overlapping bounding boxes.
- Use this for each object class that the NN is setup to predict.

Anchor Boxes:
- Used to detect multiple objects in the same grid cell.
- Detected object is assigned to a grid cell that contains the object's midpoint and anchor box for the grid cell with highest IoU with the object's shape. (grid cell, anchor box)

Training dataset anatomy:
- vertical grid cells x horizontal grid cells x #anchors x (5 + #classes). 5: (Pc, bx, by, bh, bw)

Semantic Segmentation & U-Net:
- The YOLO (You Only Look Once) algorithm shrinks the Nh and Nw. This reduces the spatial information.
- Assign every single pixel to a class.
Transpose Convolution:
- Output dimension is greater than the input dimension.
- Apply the filter on the padded output.
- Convolve input with the filter, fill the element-wise multiplication result in the respective output cell. Ignore the pad cells. For any overlapping cell, due to stride, add the cell values.

======================================================================================================================================================================================================
Face Recognition:
- Face verification supervised learning:
  - 1:1 mapping
  - A 2-step authentication process:
    (i) Identiy who you are. For example, name, ID, etc.
    (ii) Check if the face matches the identity in step (i)
- Face Recognition:
  - 1:K mapping
  - Has a database of K persons.
  - Ouput ID if the image is found in the database.
One-shot learning:
- Learn from ONE example to recognize the person again.
- Problems:
(1) Smalle dataset not effective to train a convnet.
(2) New person requires retraining the convnet.
Solution: 
Learn a "similarity" function:
- d(img1, img2) = degree of diff between 2 images
- <= threshold : "same"
- > threshold: "different"
Siamese Network:
- The 2 networks share the parameters and have the same architecture.
- The final Dense layer is used to encode the input image. For example, (128 , 1) vector, f(x).
- d(x1,x2) = |f(x1) - f(x2)| ** 2
- Example system: DeepFace
- To encode:
(1) Use triplet loss function:
  - Anchor image, postive image, negative image. These triplets are picked from the training dataset. (ð´(ð‘–),ð‘ƒ(ð‘–),ð‘(ð‘–)) is used here to denote the ð‘–-th training example.
  - d(A,P) <= d(A,N)
    d(A,P) - d(A,N) <= 0
  - To prevent the NN from learning a trivial solution (0 - 0 <= 0) setting all the parameters to 0, use "<= 0 - alpha"
  - d(A,P) - d(A,N) + alpha <= 0
  - L(A,P,N) = max(sum((f(A) - f(P))^2) - sum((f(A) - f(N))^2) + alpha, 0); sum over the image instances. 
  - J = sum(L(A,P,N))for all the triplets over the image corpera and #persons. Example, 10k pictures of 1k persons.
  - Needs multiple pictures of each person.
  - Caveat: d(A,N) is very easily large number with random pairs of images. To train the NN effectively, choose pairs which result in d(A,P) ~= d(A,N). Ideas from FaceNet.
(2) Use binary classification:
  - Linear Regression
  - y^ = sigmoid(w * sum(|f(x1) - f(x2)|) + b) for i=1...128
  - In DeepFace, the (f(x1) - f(x2))^2 / (f(x1) + f(x2)) is used in the summation.

Nerual Style Transfer:
- Content picture (C) + Styling picture (S) = Generated Image (G)
- Let a convnet learn the features in a input picture.
- Pick a neuron in a layer, find the nine image patches that maximize the neuron's activation. Repeat for other neurons.
- Cost function J(G) = alpha * Jc(C, G) + beta * Js(S, G)
(1) Initialize G with a seed, (100, 100, 3). G = G - alpha/2G * J(G)
(2) Use Gradient Descent to minimize J(G). Update the generated image pixel with G = G - alpha/2G * J(G).
- Jc(C, G) - Use middle layer to compute content cost.
  - a(l,c), a(l,g): activation of layer l on images
  - Jc(C, G) = sum(|a(l,c) - a(l,g)| ** 2)
- Styles definition: Correlation among the activations across channels in the particular layer used to calculate Js(S, G).
  Style Matrix (also called a "Gram matrix.") to capture the style of the input image (S and G):
  - In linear algebra, the Gram matrix G of a set of vectors (v1,â€¦,vn) is the matrix of dot products, whose entries are Gij=viT * vj=np.dot(vi,vj).
    In other words, G(gram)ij compares how similar vi is to vj: If they are highly similar, you would expect them to have a large dot product, and thus for G(gram)ij to be large. G(gram) = A @ A.T. The value G(gram)i,j measures how similar the activations of filter i are to the activations of filter j.
    The diagonal elements of G(gram)ii measure how "active" a filter is. For example, suppose filter i is detecting vertical textures in the image. Then G(gram)ii measures how common vertical textures are in the image as a whole. If G(gram)ii is large, this means that the image has a lot of vertical texture.
    By capturing the prevalence of different types of features (G(gram)ii), as well as how much different features occur together (G(gram)ij), the Style matrix Ggram measures the style of an image.
  - Compute the Style matrix by multiplying the "unrolled" filter matrix with its transpose: 
    - (Nc x (Nh x Nw)) @ (Nc x (Nh x Nw)).T = (Nc x (Nh x Nw)) @ ((Nh x Nw) x Nc) => (Nc, Nc)
  - a[l](i,j,k) = activation at (i,j,k) in lyaer l. Compute G[l] which is a matrix of Nc[l] x Nc[l].
  - G[l, S](k,k') = sum(sum(a[l](i,j,k) * a[l](i,j,k'))) for all i,j (height, width) over all k channels for style picture
  - G[l, G](k,k') = sum(sum(a[l](i,j,k) * a[l](i,j,k'))) for all i,j (height, width) over all k channels for generated picture
  - J[l](S, G) = |G[l, S](k,k') - G[l, G](k,k')| ** 2 = sum(sum(G[l,S] - G[l, G])) / (2 * Nh *Nw * Nc) for all channels in layer l.
  - For bettter styling effect, use J[l](S, G) from multiple layers. Js(S,G) = sum(lambda * J[l](S, G)) for some number of layers. This takes both low-level and high-level correlations into account.
  - J(G) = alpha * J(C, G) + beta * J(S, G). Use gradient descent to generate G which minimizes J(G).

Get better results if you "merge" style costs from several different layers.
Each layer will be given weights (Î»[l]) that reflect how much each layer will contribute to the style.
How do you choose the coefficients for each layer? The deeper layers capture higher-level concepts, and the features in the deeper layers are less localized in the image relative to each other. So if you want the generated image to softly follow the style image, try choosing larger weights for deeper layers and smaller weights for the first layers. 
In contrast, if you want the generated image to strongly follow the style image, try choosing smaller weights for deeper layers and larger weights for the first layers.
What you should remember:
The style of an image can be represented using the Gram matrix of a hidden layer's activations.
You get even better results by combining this representation from multiple different layers.
This is in contrast to the content representation, where usually using just a single hidden layer is sufficient.
Minimizing the style cost will cause the image G to follow the style of the image S.
======================================================================================================================================================================================================
L2 Distance
- Euclidean distance
- A measure of the straight-line distance between two points in a multi-dimensional space.
- sqrt(sum((A - B) ** 2))
- dist = numpy.linalg.norm(a-b, ord=2)
======================================================================================================================================================================================================
Sequence Models
Use Cases:
(1) Speech Recognition. Input: speech Output: text
(2) Music Generation. Input: a number, genre, etc. Output: Music notes. One-to-many.
(3) Sentiment Classification. Input: sentence, Output: integer count. #likes or #stars. Many-to-one.
(4) DNA Sequence Analysis. Input: sequence of A,C,G,T Output: which part of the sequence corresponds to a specific pattern, protein for instance.
(5) Machine Translation. Input: sentence in source language Output: sentence in translated language. 2 parts: Encoder reads the input; Decoder outputs the translated sentence.
(6) Video Activity Recognition. Input: sequence of video frames Output: activity description.
(7) Name Entity Recognition. Input: text Output: people / entities identified in the input.

Notation:
Both input and output are one-hot encoded vectors.

Standard NN (one-to-one) work well for sequence because of 2 problems:
(1) Inputs and outputs can be of different lengths in different samples of the dataset.
(2) Standard NN doeesn't share features learnt across different positions of text. For example, in name-entity recognition, if a name is identified in early in the sequence, the same string (faeature) should be identified as well in later parts of the sequence.
    - Same as how ConvNet help sharing features learnt in one part of an image to generalize quickly to other parts of the image, Recurrent NN plays the same role for sequence modeling.

Recurrent NN:
- Predict the next steps based on the knowledge of all prior steps.
- Uses activations up to the current RNN cell/time step to compute output y-hat.
- At each RNN cell/time step, a(t-1) and x(t) are input, a(t) and y(t) are output. Many-to-many.
- Tx = length of input; Ty = length of output
- a0 = vector of zeros.
  Wax = weights used to compute output(a) using x
  Wya = weights used to compute output(y) using a
  a(t) = g(Waa @ a(t-1) + Wax @ x(t) + ba) <= Usually tanh is used. Sometimes ReLU
  y^(t) = g(Wya @ a(t) + by) <= Sigmoid fo binary classification, softmax for multi-class
- Simplify a(t) = g(Waa @ a(t-1) + Wax @ x(t) + ba) = g(Wa @ [a(t-1), x(t)] + ba)
  Example:
  - if a(t-1) is 100-element vector, Waa would be (100, 100)
  - if x(t) is 10,000-element vector, Wax would be (100, 10000)
  Wa is a matrix of [Waa | Wax] stacked horizontally with shape (100, 10100)
  [a(t-1, x(t))] is a matrix of [a(t-1) | x(t)] stacked vertically with shape (10100, 1)
- Simplify y^(t) = g(Wya @ a(t) + by) = g(Wy @ a(t) + by)
- L(t)(y^(t), y(t)) => use BinaryCrossEntropy loss function
  L(y^(t), y(t)) = sum(L(t)(y^(t), y(t))) for all time steps.
- Backpropagation through time using gradient descent to update parameters.
language Model and Sequence Generation:
- L(y^(t), y(t)) = -sum(y(t) * log(y^(t)))
  L = sum(L(y^(t), y(t))) for all time steps.
- P(y(1), y(2), y(3)) = P(y1) * P(y2 | y1) * P(y3 | y1,y2)
Sampling a sequence from a trained RNN:
- Generate randomly chosen sentence from RNN.
- y^(t): numpy.random.choice(P(word1)P(word2)P(word3)...)

Basic RNN is not very good at capturing long-range dependencies. This is similar to the vanishing gradients problem inherent in training very deep conventional NN.
Exploding gradients can also happen which is easily spotted because the parameters blow up which result in some NaN due to numerical overflows. Use gradient clipping to solve it. If gradient vectors larger than some threshold, rescale them.

Gated Recurrent Unit:
- Modification of RNN
- Helps RNN capture long-range dependencies and solve the vanishing gradient problem.
- C = memory cell
- It outputs an activation a(t)
- C(t) = a(t) : [1, N] vector. Each index serves to remember different types of contexts. For example, singular/plural, food, places, etc
- C~(t) = tanh(Wc @ [C(t-1), x(t)] + bc) <= candidate replacement of current C(t) <- simplified version
- Relevance Gate [0, 1], Gr = sigmoid(Wr @ [a(t-1), x(t)] + br) : 1: Update 0: Don't update
- C~(t) = tanh(Wc @ [Gr * C(t-1), x(t)] + bc) <= candidate replacement of current C(t) <- Full version
- Update Gate [0, 1], Gu = sigmoid(Wu @ [a(t-1), x(t)] + bu) : 1: Update 0: Don't update
- C(t) = Gu * C~(t) + (1 - Gu) * C(t-1) : element-wise multiplications.
- y^(t) = g(Wya @ a(t) + by) usually softmax
Long Short Term Memory (LSTM)
- Modification of RNN
- More powerful than GRU
- C~(t) = tanh(Wc @ [a(t-1), x(t)] + bc) <= candidate replacement of current C(t) <- Full version
- Update Gate [0, 1], Gu = sigmoid(Wu @ [a(t-1), x(t)] + bu) : 1: Update 0: Don't update
- Forget Gate [0, 1], Gf = sigmoid(Wf @ [a(t-1), x(t)] + bf) : 1: Forget 0: Don't forget
- Output Gate [0, 1], Go = sigmoid(Wo @ [a(t-1), x(t)] + bo) : 1: Forget 0: Don't forget
- C(t) = Gu * C~(t) + Gf * C(t-1) : element-wise multiplications.
- a(t) = Go * tanh(C(t))
- y^(t) = g(Wya @ a(t) + by) usually softmax

Bidirectional RNN:
- Both forward and backward passes can by RNN or GRU or LSTM.
- y^(t) = g(Wy * [a(t), a'(t)] + by): a'(t) is the backward pass activation value.
- LSTM BRNN is commonly used in NLP.
- Needs the complete sequence to work. For example, if used in speech recognition system, have to wait for the speaker to stop talking to start processing the sentences.
======================================================================================================================================================================================================
NLP & Word Embeddings
Word Embeddings is a featurized representation of the words.
Learn concepts / features of each word to draw similarity, etc.
Orignal model trained on large corpera of texts, use transfer learning for other tasks which have smaller set of training dataset.

Analogies using word feature vectors.
Analogy reasoning.
- Man is to woman as King is to Queen.
e(man) - e(woman) ~= e(king) - e(?)
Find w which maximizes similarity(e(?), e(king) - e(man) + e(woman))

similarity function:
(1) Cosine similarity
(2) Squared differences || U-V || ** 2
E: Embeddings matrix. (feature size, vocab size)
O: one-hot encoding vector of each of the word in the vocabulary. (vocab size, 1)
e(word) = E @ O -> (feature size, 1)

To build a language model, use last few words
To learn a word embedding, use any of the following:
(1) #words on the left & right
(2) Last 1 word
(3) Nearby 1 word - Skip-grams model.

Word2Vec
Skip-grams model
- Use Supervised learning to learn good word embeddings.
- Randomly choose a context word
- Randomly choose another word within some window size from the context word as the target word.
- So, set up a supervised learning problem where given a context word, predict what is the randomly chosen word within a windows size of the context word.
- The goal of this supervised learning is not to optimze the learning problem per se, but to use this learning problem to learn good word embeddings.
- The supervised learning problem is to learn to map Context (Input x), c to Target (Output y), t
- O(c) -> E -> e(c) = E @ O(c) -> softmax -> y^
- softmax: p(t|c) = exp(theta(t).T @ e(c)) / sum(theta(j).T @ e(c)); theta(t) = parameter associated with output t, i.e., the chance of output t being the label.; sum over the vocabulary size.
- L(y^, y) = - sum(y * log(y^)) over the vocabulary size.
- y: (vocabulary size, 1)
- Problems: The denominator, sum(theta(j).T @ e(c)), is slow.
  - Use binary tree to solve it. Hierarchical softmax. Time complexity is log(N) instead of N.
  - The tree is organized with commonly used words close to the root of the tree, less common ones close to the leafs.

Negative Sampling:
- Sample a context and target word.
- X: Context word (c), target word (t)  Y: 1/0
- Positive example Randomly sample a target word in a window size from the context word.
- Negative examples (k): pick a word randomly from the vocabulary. k = 5-20 for small datasets, 2-5 for large datasets.
- Logistic regression of P(y=1 | c, t) = sigmoid(theta(t).T @ e(c))
- Negative : Positive = 5:1
- vocabulary size sigmoid is cheaper than softmax. In every iteration, only train on k+1 input.
- Sampling target words for negative examples based on P(wi) = (f(wi)^(3/4)) / (sum(f(wj)^(3/4))) sum over the vocabulary size.

GloVe (Global vectors for word representation)
- Not used as much as SKip-grams and Negative Sampling.

Machine Translation can be thought of as a "conditional language model".
- Instead of start of with a vector of zeros, start off with the encoded source language.
- The encoded source language serves as a(0) to the RNN/sequence model.
- Instead of predicting the probability of any novel sentence, it predicts the probability of translated sentence conditioned on the input source language.
- P(y|x), x: source language sentence, y: translated language sentence, both are embedding vectors.
- Maximize P(y|x). Greedy search word by word is exponentially large depending on the vocabulary size.
Beam Search:
- Approximate / heuristic search algorithm.
- Beam width, B, best #choices to consider int every time step. Use B copies of the network for parallel calculation in every time step.
- B=1 reduces this to greedy search.
- First step: P(y1|x) y1 is [word1, word2, word3], length of it determined by B
- Second step: P(y1,y2|x) = P(y1|x) * p(y2|x,y1)
- Arg Max sum(P(yt|x,y1,...yt-1)) for all time steps, t1...Ty. Ty = #words in output sentence.
- P(yt|x,y1,...yt-1) will result in very small floating point number which will cause accuracy issue - numerical rounding error.
- Second problem is that this algorithm will incline towards shorter-sentence predictions since fewer factors of P(|) will result in larger number compared to longer-sentence predictions.
- Arg Max sum(log(P(yt|x,y1,...yt-1))) fixes the numerical rounding issue.
- Arg Max sum(log(P(yt|x,y1,...yt-1))) / (Ty ^ alpha) -> fixes the long-sentence penalty issue.
Error Analysis:
- B or RNN at fault?
- Best: y*; prediction: y^
(1) Compute P(y*|x) and P(y^|x)
(2) P(y*|x) > P(y^|x): This aligns with the result but Beam search choise y^. So, it is at fault. Increase beam width, B.
(3) P(y*|x) <= P(y^|x): RNN computes the wrong result. So, it is at fault, the objective function.
Note: Need to consider length normalization as well (1 / Ty^alpha).
Machine Translation Evaluation:
- BLEU Score - Bilingual Evaluation, Understudy
- Given a machine generated translation, automatically compute a score that measures how good the translation is.
- Used to evaluate text-generation applications.
- Pn = sum(Count<clip>(n-gram)) / sum(Count(n-gram)) where n-gram appears in prediction sequence. Count<clip> = max count the n-gram appears in any of the reference translations.
- Pn = 1 if y^ exactly the same as any of the reference translation.
- Combined BLEU score = BP * e^(1/n * sum(Pn)) for all lengths of n-grams.
- BP, brevity penalty = 1 if MT_output_length > reference output length; 
- BP = e^(1- reference_output_length/MT_output_length) otherwise.

Naive encoder-decoder RNN is good at translating short sentences. BLEU score will drop when the source text gets longer.
Attention model helps with translating long text.
- Use Bidirectional RNN.
- a(t') = [a(t') | a_backward(t')]; t' = time step of the source/input language sequence.
- alpha(t,t') = weight / amount of attention y(t) should pay to a(t')
- C(t) = sum(alpha(t,t') @ a(t'))

Speech Recognition.
- Use spectogram.
CTC cost for speech recognition:
- Connectionist temporal classification.
- Usually output text is shorter than the input sequence. For example, with 100Hz, 10s of speech will end up with Tx = 10,000.
- For RNN Ty to match Tx, use repeated character and collapse the repeated characters not separated by "space"/"blank".
- Example: "The quick brown fox": ttt_h_eee____ ___qqq__uuu__ -> The qu
======================================================================================================================================================================================================
Transformer Network
- Eliminate the sequential nature of RNN which processes one token / word at a time.
- Ingest the entire sequence at once and process the tokens/words in parallel.
- Attention + CNN
  - Self attention
  - Multi-head attention
Self-Attention:
- A(q,K,V) = attention-based vector representation of a word.
  - Calculate for each word in the input sequence. A(1),...A(5) in parallel.
  - q: query, K: key, V: value
  - q(i) = Wq @ x(i)
  - k(i) = Wk @ x(i)
  - v(i) = Wv @ x(i)
  - x(i) is the word embedding
- W(i) is the head(i) in multi-head attention. It can be thought of as a "question" to find out the context of every word. The same head has to be dot-product with Q,K,V of every word.
  - For example, W(1) = "What's happening?" W(2) = "When?", W(3) = "Who?", etc
  - head(i) = Attention(W(i) @ Q, W(i) @ K, W(i) @ V)
  - All heads computed in parallel.
- h = #heads
- Multihead(Q,K,V) = concat(head1, head2, head3,...) @ W
Note: 
(1) In self-attention, multiply x with matrices W. In case of the multi-head attention, you don't need to do this, as you already have the matrices W(i) in each head, and you would effectively do the calculation twice if you did the multiplication here also. 
    In the simplest case of multi-headed self-attention you would actually use q = k = v = x. The reason we anyway show q, k and v here as different values is that in one part of the transformer (where you calculate the attention between the input and output) the q, k, and v are not the same as they carry different information.
(2) Transformer Details:
    - Position encoding of the word in the input sequence. This is needed in transformer architecture since the computation is done in parallel and thus losing the position information in the original sequence.
    - i repeats itself once to encode 2 dimensions using the sine and cosine functions (same i is used for sine and cosine). To calculate i, use a helper index k, which is simply counting over the dimentions of the word embedding from 0 to d-1. i = k // 2.
    i: 0
    k = 2i = 0
    k = 2i + 1 = 1
    i: 1
    k = 2i = 2
    k = 2i + 1 = 3
- Masked multihead attention is used during training to mask some texts from the input sequence so that the transformer architecture would learn to predict the masked words compared to the complete Y sequences available in the training dataset.
Positional Encoding:
- A feature that contains the information about the relative positions of words. The sum of the positional encoding and word embedding is ultimately what is fed into the model. 
  If you just hard code the positions in, say by adding a matrix of 1's or whole numbers to the word embedding, the semantic meaning is distorted. 
  Conversely, the values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted, and is instead enriched with positional information. 
  Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data.
======================================================================================================================================================================================================
ML Strategy:
Orthogonalization: What to tune to achieve a specific effect.
(1) Fit training  set well on cost function
    - Bigger network
    - Different optimizers
    - NO: early stopping - This is not orthogonal as it will affect (2) below
(2) Fit dev/validation set well on cost function
    - Regularization
    - Bigger training set
    - NO: early stopping - This is not orthogonal as it will affect (1) above
(3) Fit test set well on cost function
    - Bigger dev/validation set
(4) Performs well in real world
    - Change dev/test set or cost function.
  
Use a single number evaluation metric
Classifier:
- 2 metrics: Precision and Recall.
- Sometimes different classifier models will yield different scores for these 2 metrics.
- Instead of using 2 to identify the "best" classsifier to use, use a single metric - F1 score.
Satisfying and Optimizing metrics:
- If N metrics, pick ONE to optimize, the rest could be just satisfying.
- Example: 
  (1) Optimize accuracy, Satisfiies running time.
  (2) Optimize accuracy, Satisfiies #false positive. Example in the use case of wake/trigger words, <= 1 false positives in every 24 hours.
  
Dev/test datasets:
- Dev / cross-validation datasets is used to evaluate different ideas and pick one. For example, used to tune hyperparameters and/or to evaluate the best performing model.
- Must come from the same distribution. In other words, the bull-eye's target must be the same for both datasets.
- It is ok for the training dataset to come from a slightly different distribution than the dev/test sets. For example, scrapping the dataset from the internet or purchase from third-party vendors.
- The goal of test set is to evaluate how good the final system is. So it is best to get these datasets from the field - data the system is built for.
How about size?
- Test set should be big enough to give high confidencein the overall performance of the system.
- Train-test split - The "test" is actually the dev/cross-validation dataset.
  - It's ok to not have a test dataset if ensuring "high confidence" in the final system is not critical.
- Must make sure large percentage of dev/test set to be as close to the field as possible.
- Example, 200,000 corpera of data scrapped from internet. 10,000 from the field.
  - Bad idea: Mix them, shuffle and distribute them to train/dev/test. This will result in dev/test having big proportion of data not of concern - away from the bulleye's target.
  - Recommended: Training: 200,000 + 5000 from the field. dev/test 25,000 each.
  
Error metric:
- Keep reviewing error metrics.
- Example: Error (J) = sum(Wi * {y^(i) != y(i)}) / sum(Wi) for all i in the dataset.
                      W(i): 1 if desirable input data; 10 if non-desirable input data.
- If doing well on metric + dev/test does not translate to doing well on final application, change metric and/or dev/test set.

Bayes optimal error - Best possible error. A theorical level which is impossible to achieve for any X->Y function, F(Y|X).
Progress is often quite fast until the system surpasses human-level performance. After that it would slow now / plateau. The reason is that it would be difficult to have a good estimate of Bayes error that still helps with making good decision clearly.

So long as ML is worse that humans,
- Get labelled data from humans.
- Gain insight from manual error analysis - Why / how does a person get it right?
- Better analysis of bias/variance.

For tasks that humans can do really well, human-level error is a proxy to Bayes error. Human error ~= Bayes error.

Example 1:
Human (~ Bayes): 1%
Training error: 8%
Dev error     : 10%
Focus on bias:
- Bigger model
- Train longer
- Bettern optimization algorithms: Add momentum or RMSProp or use bettern algo like Adam.
- Review / change NN architecture / hyperparameters search - RNN, CNN

Example 2:
Human (~ Bayes): 7.5%
Training error: 8%
Dev error     : 10%
Focus on variance:
- More training data
- Regularization - L2, Dropout, Data augmentation.
- Review / change NN architecture / hyperparameters search - RNN, CNN

Training error - human error = Avoidable bias
Dev error - Training error = Variance

Problem areas where ML significantly surpasses human-level performance:
- Structured data
- Lots of data - Easier for computer to find statistical patterns
Problem areas harder for ML to surpass human-level performance:
- Natural perception: Speech, image recognition

Carry out Error Analysis on dev/cross-validation dataset
- Count up #errors from various different categories.
  Example: cross-validation error is 10%. Analyze the distribution of this 10% of misclassified pictures. Let's say 100 pictures are misclassified and 5 of them are of alien categories. Spending tine and effort to work on this 5% misclassified category will only give 0.5% improvement on the dev set.
- Consider examining examples the algorithm prediction was right and wrong. Examining only the wrong examples will end up with biased estimates of the errors.

Mislabelled datasets
- DL algorithms are quite robust to random errors in the training set. If the total dataset size is big and the actual percentage of error not too high, it is ok to leave it.
- If mislabelled in dev/test sets, carry out error analysis.

Build first system quickly and dirty, then iterate, except:
- Have significant prior experience in the application area.
- Significant body of academic literature that can be drawn on for pretty much the same problem the system is built for.

If training data comes from different distribution than that of dev/test set and error differs significantly,
- Can't tell if it is variance problem.
- Maybe training data is high-resolution and/or dev dataset is more difficult to deal with.
- To help identify the root cause, 
  (1) Carve out a training-dev set from the training dataset. 
  (2) Train on the remaining training dataset.
  (3) Check variance between training and train-dev set. If big, there is confirmed a variance problem.
      Otherwise, if small error diff between training and training-dev but big error difference between training-dev and dev, then there is a data mismatch problem.
      Or maybe both "Avoidable bias" problem between training and training/dev + data mismatch between original training data and the dev/test set.
      - Training set - human error level = avoidable bias
      - Training-dev - training = variance
      - Dev - training-dev = data mismatch if > training/dev error. If smaller, it may mean that the training datset is much harder than the dev/test dataset.
      - Test - Dev = test = degree of overfitting to the Dev dataset. May need to find a bigger Dev dataset.

              General Application               Final Application
              (Source of training data corpera) (Field/target dataset)
Human level         4%                              6%
Training error      7%                              6.5%
Dev/test error     10%                              7%

Addressing data mismatch error:
- Carry out manual error analysis to understand diff between training and dev dataset. Only on dev but exclude the test to avoid overfitting.
- Make traiing dataset more similar to the dev/test set. Or collect more data similar to dev/test set.
- Generally, just find ways to close the gap between training and dev/test set. For example, using artifically data synthesis. The proportion of the synthesized data must match up to the size of the real data. Otherwise, ML will overfit to it. For example, noisy background sound, graphically synthesized images.