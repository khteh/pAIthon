Conditional Probability:
P(a|b) = P(a ^ b) / P(b)
P(a ^ b) = P(a|b)P(b)
P(a ^ b) = P(b|a)P(a)

P(a|b)P(b) = P(b|a)P(a)
P(a|b) = P(b|a)P(a) / P(b) <= Bayes' Rule
P(b|a) = P(a|b)P(b) / P(a) <= Bayes' Rule

Bayes' Rule: 
Knowing - P(visible effect|unknown cause)
We can conclude - P(unknown cause | visible effect)

Example:
Knowing - P(medical test result | disease)
We can conclude - P(disease | medical test result)

If Probability of 'a' and 'b' are independent,
P(a ^ b) = P(b)P(a)

P(aâˆ£b) = P(bâˆ£a)P(A) / P(B)

Bayes rule/formula:
Prior Odds: 5:95
Likelihood Ratio: P(A|B) / P(A|C) : (80/100) / (10/100) = (80/100) * (100/10) = 80/10 = 8
Posterior Odds: 40:95

Joint Probability:
P(A|b) = P(A, b) / P(b) : Comma signifies ^
       = alpha * P(A, b), where alpha = 1/P(b) which is just some constant.
       = alpha * <Probability distribution of 'b' which sums up to 1>
=> Conditional probability is proportional to Joint probability

Inclusion-Exclusion:
P(a v b) = P(a) + P(b) - P(a^b)

Marginalization: Calculate probability of independent variable based on given/known joint probabilities.
P(a) = P(a,b) + P(a, Not(b))
P(X = xi) = Summation over all the values (j) y could take on of P(X = xi, Y = yj)

Conditioning: Calculate probability of independent variable based on given/known conditional probabilities.
P(a) = P(a|b)P(b) + P(a|Not(b))P(Not(b))
P(X = xi) = Summation over all the values (j) y could take on of P(X = xi|Y = yj)P(Y = yj)

Bayesian Network: Data structure which represents the dependencis among random variables.
- Directed graph
- Each node represents a random variable.
- Arrow from X to Y means X is a parent of Y
- Each node X has a probability distrubution P(X | Parents(X))

Inference:
- Query X: variable for which to compute distribution
- Evidence variables E: observed variables for event e
- Hidden variables Y: non-evidence and non-query variable
- Goal: Calculate P(X | e) where e could be more than 1.

Inference by Enumeration:
P(X | e) = alpha * P(X,e) = alpha * Summation over all the values (j) y could take on of P(X, e, y)
X: query variable
e: evidence
y: ranges over values of hidden variables
alpha: normalizes the result.
Cons: Time complexity grows with complexity of the network

Aproximate Inference:
- Sampling:
  - Rejection Sampling: Filter out samples which do not match event specification
  - Likelihood Weighting:
    - Start by fixing the values of evidence variables.
    - Sample the non-evidence variables using conditional probabilities in the Bayesian Network
    - Weight each sample by its likelihood: the probability of all of the evidence.

Markov Assumption: The assumption that the current state depends on only a finite fixed number of previous states.
Markov Chain: A sequence of random variables where the distribution of each variable follows the Markov assumption.
Hidden Markov Model: a Markov model for a system with hidden states which generate some observed events.
Sensor Markov Assumption: the assumption that the evidence variable depends only on corresponding state.

Supervised Learning > Classification:
Perceptron Learning Rule: Given data point (x,y), update each weight according to: w =  w + alpha*(y - h(X)) * x
Start with random weights, learn from data and update the weights that result in better weight vector which reduces loss.

Support Vector Machines:
  Maximum Margin Separator: Boundary that maximizes the distance between any of the data points
If the data is not linearly separable, it works from higher dimension to find the separation boundary. For example, other shapes than linear lines.

Optimization Problem Formulation:
- Local Search: 
  - Algorithms: Hill-Climbing, Simulated Annealing
- Linear Programming
  - Algorithms: 
- Constraint Satisfaction
  - Algorithms: AC3, Backtracking

Supervised Learning > Regression:
Learning a function mapping an input point to a continuous value.

Types of loss functions:
0-1 Loss: Used in discrete classification
L1 Loss: |actual - prediction| -> Used in continuous number prediction. Used when we don't care about outliers.
L2 Loss: (actual - prediction)^2 -> Penalizes worse / bigger loss more harshly. Used when we care about outliers.

Underfitting (High bias):
- J(train) AND J(cv) are high
- The model has a strong preconception of how the data  should fit into itself. For example, linear function, etc.
- More data will NOT help.
- Example: Training dataset performance: 64%, Test dataset performance: 47%

Overfitting (High variance):
- J(train) is low; J(cv) is high
- A model that fits too closely to a particular data set and therefore may fail to generalize to future data. High variance because if the model overfits, a tiny change in one of the training data set will end up completely different model.
- This is a side effect of minimizing loss of a model. If loss = 0, the model may only work on the specific data set.
- One way to counter this problem is add other parameters to optimization. For example, consider complexity:
- More data is likely to help.
- One of the causes is data leakage - some of test data leak into training data.
- Example: Training dataset performance: 93%, Test dataset performance: 99%
Note: Overfitting is sometimes a good thing to begin with. It means the model is learning.

High bias AND high variance:
- J(train) is high; J(cv) higher than J(train)
- Sometimes happen in NN. Less in linear regression.
- Doesn't really happen for linear models applied to 1-D.
- Underfit and overfit on parts of the data at the same time.

Balanced (Goldilocks zone):
- Example: Training dataset performance: 98%, Test dataset performance: 96%

Cost(h) = Loss(h) + w*Complexity(h) : 'w' gives weight to the complexity

Adding the term w*Complexity(h) is called "Regularization": Penalizing hypotheses that are more complex to favour simpler, more general hypotheses
To overcome overfitting:
(1) Collect more training samples / observations.
(2) Use fewer features.
Many features + insufficient data -> overfitting
(3) Regularization - Gently reduces the impact of some of the features without eliminating them outright. It encourages the model to shrink the values of the parameters without necessarily setting them to 0 which eliminates the features.
    Cost(w,b) = (sum((f_w_b(x) - y) ** 2) + lambda * sum(w[j] ** 2)) / 2m
    f_w_b <= Linear regression modal
    sum((f_w_b(x) - y) ** 2) <= Mean squared error. Fit data
    sum(w[j] ** 2) <= Regularization term. Keep w[j] small
    lambda balances both goals
    Example: f_w_b = w1x + w2x^2 + w3x^3 + w4x^4 + b
    lambda = 0: Overfitting. Wiggly curve/graph which tries to fit every single data point in the training data set.
    lambda = 10^10: f_w_b = b Underfitting
Gradient Descent withh regularization:
w = w - alpha/m * (sum((f_w_b(x) - y) * x) + lambda * w[j])
  = w * (1 - alpha * lambda / m) - (alpha / m ) * sum((f_w_b(x) - y) * x)
  alpha: 0.01, lambda: 10, m: 100 => (1 - alpha * lambda / m) = (1 - 0.001) = 0.999
  This shows that every iteration of gradient descent, regularization reduces the value of w, thus reduces the impact of the feature.

Strategy to debugging a learning algorithm:
(1) Get more data - Fixes high variance.
(2) Try smaller sets of features (Simplify polynomial equation by removing some of the higher order components.) - Fixes high variance.
(3) Add more features - Fixes high bias
(4) Add polynomial features - Fixes high bias
(5) Decrease lambda - Fixes high bias
(6) Increase lambda - Fixes high variance.

Hold-out cross validation splits data into training and testing data sets. How to split?
k-fold corss-validation: Splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data as training set.

Feature Scaling: When a feature has a large range of values, the weight/parameter calculated by the model will have small range. In this case, a small change in weight will have big change in the cost value.
This causes the gradient descent to take longer time to find it's global minimum, i.e., converge slower. To oversome this, scale the input data. 2 ways:
(1) Divide by the maximum value of the data range. This will produce scaled input data of [0, 1]
(2) Mean normalization: Find the average value (ave). (x - ave) / (Xmax - Xmin). This will produce scaled input data of [-1, 1] which centers around 0.
(3) Z-score normalization: Find the standard deviation (d) and the mean. (x - ave) / d.
    Mean: sum(data) / len(data)
    standard deviation: sum((data[i] - mean) ** 2) / len(data)
Rules of thumb: Aim for [-1, 1] input data range. These need to rescale:
(1) [-100 , 100] : Too large
(2) [-0.001, 0.001] : Too small
(3) [98.6, 105]: Too large. Example, Fahrenheight body temperature.
Notes:
Feature scaling usually isn't required for your target variable (labels).
Feature scaling is usually not required with tree-based models (e.g. Random Forest) since they can handle varying features.

Reinforcement Learning:
Given a set of rewards or penalties, learn what actions to take in the future.
Markov Decision Process:
Model for decision-making, representing states, actions, and their rewards.
Q-Learning:
Method for learning a function Q(s,a), estimate of the value (reward) of performing action 'a' in state 's'. Start with Q(s,a) = 0 for all s,a.
Q(s,a) <- Q(s,a) + w*(new estimate - old estimate): w:0 old values are more important; w:1 new values are more important.
Q(s,a) <- Q(s,a) + w*((r + future reward estimate) - Q(s,a)) 
Q(s,a) <- Q(s,a) + w*((r + w1*max(Q(s',a') for all a')) - Q(s,a)): w1 provides weights for future vs current reward

Balance between Exploration and Exploitation.

Function Approximation:
Approximating Q(s,a), often by a function combining various features, rathen than storing one value for every state-action pair.
Similar to depth-limiting approach in MiniMax. Used when it is not feasible to explore all the posible values of Q(s,a) in a bigger state space.

Unsupervised Learning:
Given input data without any additional feedback (label), learn patterns, find structure in the data.
Tasks:
(1) Clustering: Organize a set of objects into groups in such a way that similar objects tend to be in the same group. Example: Genetic research, Image segmentation, Market research, Medical imaging, Social network analysis.
    One technique is k-means clustering: Algorithm for clustering data based on repeatedly assigning points to clusters (k number of clusters) and updating those clusters' centers
(2) Anomaly Detection: Find unusual events in the data.
    Use case: Fraud detection.
(3) Dimensionality Reduction: Compress large corpuses of data to a much smaller dataset without losing key / important information.

k-means clustering:
(1) Choose a number for 'k' cluster centroids, u[k]. k < len(data)
(2) Randomly assigns 'k's values which have the same dimension as the dataset.
(3) Assign samples to the nearest centroid (k)
    for i of range(len(data)):
      c[i] = index (from 1 to k) of cluster centroids closest to x[i]
    The distance can be calculated with min(|x[i] - u[k]|), or more generally, min(|x[i] - u[k]|  ** 2 )
(4) Move the cluster centroids to the average of the points
    for i of range(k):
      u[i] = average (mean) of points assigned to cluster k
    The means can be calculated with sum(x[]) / len(x[])
There will be situations where one of the uk has zero data points assigned to it. In this case, either
(1) Remove the empty cluster: k = k - 1
(2) Reinitialize the cluster centroids and restart the algorithm.
Optimization objective:
c[i] = index of cluster (1,2,...,k) to which sample x[i] is currently assigned to.
uk = cluster centroid k
uc[i] = location of centroid k to which x[i] is currently assigned to.
cost function: J(c, uk) = sum(|x[i] - u[k]|  ** 2 ) / m
Every step along the way reduces J(c, uk).
To minimize local minimum and choose the best cluster centroids, use multiple random initializations and choose the cluster with the lowest J(c,u). i.e., run the algorithm multiple times, maybe 50 to 1000.

Anomaly Detection:
Density Estimation: Model p(x) from data. Probability of x[i] being seen in the dataset.
p(x[i]) < epsilon : Anomaly
p(x[i]) >= epsilon: OK
p = Gaussian distribution - Needs mean and sigma (standard distribution); sigma ** 2 is variance
To model p(x) with multiple featues (N), every feature is a Gaussian distribution.
(1) Calculate the mean and sigma of every feature.
(2) p(x) = p(x1; u1,s1) * p(x2; u2,s2) * p(x3; u3,s3) * ... * p(xN; uN,sN)

Anomaly Detection vs Supervised Learning:
Anomaly Detection is mostly used when:
(1) Very small number of positive exaples (y=1). 0-20 is common. Large number of negative (y=0)
(2) Many different "types" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous samples seen so far (training set)
Examples: Security related applications like fraud, defects detection of manufacturing very complex products like the aircraft engines, monitoring complex systems like in a data center.

Supervised Learning:
(1) Large number of positive and negative examples
(2) Enough positive examples for algorithm to get a sense of what positive examples are lke; future positive samples likely to be similar to ones in training set.
Examples: Email spam classification, defects detection of mobile phone manufacturing, weather prediction, disease classification.

Anomaly Detection feature selection:
(1) Choose feature with gaussian distribution property (plt.hist(x)). Otherwise, try to transform it:
    (i) log(x + C) : Lower values of C transform better
    (ii) sqrt(x)
    (iii) x ^ 1/3

Anomaly Detection error analysis:
Most common problem: p(x) is comparable for both normal and anomalous samples. p(x) is large. In this case, the algorithm will fail to flag an anomalous test data. Solutions:
(1) Check the anomalous test data. Create new feature which helps detect the anomaly.
(2) Create new feature based on existing ones using simple equations like x1 / x2 or (x1^2) / x2, etc.

Principal Component Analysis:
(1) Commonly used for visualization. Example: Take data with many features, say 50, 1000 or even more, reduce the #features to a few so as to enable visualization / plotting.
    - Common used by data scientists to visualize data for analysis.
    - Choose feature(s) with meaningful degree of variation.
    - Create new feature(s) based on existing ones. Example: length x height
(2) Features should first be normalized to have zero mean before applying PCA.
(3) Features should first be scaled before applying PCA.
(4) It will choose an axis(the Principal Component) around the origin which results in the largest variance when projecting the data onto it (at 90 degree angle).
    - All new axes created will be perpendicular to one another (90-degree)
(5) Less common applications:
    (i) Data compression - Reduce storage and/or transmission cost
    (ii) Speed up training of a supervised learning model.
Neural Network (multilayer perceptron):
Activation functions:
(1) Step function: g(x) = 1 if x >=0, else 0
(2) Logistic Sigmoid: g(x) = e^x / (1 + e^x)
(3) Rectifier Linear Unit (ReLu): g(x) = max(0,x)
Choices of Activation Function:
(1) Output layer: Depends on the nature of the label:
    (i) Binary classification: Sigmoid
    (ii) -/+: Linear. For example, stock price change 
    (iii) Only positive numbers: ReLU
(2) Hidden layers: Mostly use LeLU. Reasons:
    (i) Simpler calculation compared to Sigmoid. This makes NN runs faster.
    (ii) There are 2 horizontal asymtotes in Sigmoid. This results in derivaties being close to 0 and therefore make gradient descent slower to run to global minimum.
    Do NOT use linear activation function. This will result in a linear regression and defeats the purpose of NN.
    The "off" or disable feature  of the ReLU activation enables models to stitch together linear segments to model complex non-linear functions.
Types of Layers:
(1) Dense: Every neuron in a layer computes it's output / activation using ALL inputs/activations from ALL neurons in previous layer.
(2) Convolution: Neurons only look at regions of the data. Benefits:
    (i) Faster computation.
    (ii) Need less training data - less prone to overfitting.
    
h(x1,x2) = g(w0 + w1x1 + w2x2 + ...)
Example:
(1) To mdel the OR function:
    h(x1,x2) = g(-1 + x1 + x2); w0 = -1, w1=w2=1
(2) To mdel the AND function:
    h(x1,x2) = g(-2 + x1 + x2); w0 = -2, w1=w2=1
Training - to calculate the parameters:
- Trade-offs between speed and accuracy
(1) Gradient Descent: Algorithm for minimizing loss when training NN
    Start with a random choice of weights, repeat:
    - Calculate the gradient based on ALL data points (training data set): direction that will lead to decreasing loss
    - Update weights according to the gradient
(2) Stochastic Gradient Descent: Same as Gradient Descent but "... based on ONE data point"
(3) Mini-Batch Gradient Descent: Same as Gradient Descent but "... based on ONE SMALL BATCH of data points"
Perceptron: Only capable of learning linearly separable decision boundary
Multilayer NN: Artificial NN with an input layer, an output layer, and at least one hidden layer. Capable of modelling more complex problems compared to perceptron.
To train multi-layer NN, we have to propagate the loss/error from the output back to the hidden layers:
Backpropagation: Algorithm for training NN with hidden layers
    Start with a random choice of weights, repeat:
    - Calculate error for output layer
    - For each layer, starting with output layer, and moving inwards towards the earliest hidden layer
      - Propagate error back one layer

To prevent underfitting (high bias):
(1) Use a larger network.

To prevent Overfitting (high variance):
(1) Collect more data
(2) Choose lambda appropriately.
Dropout: Temporarily removing units - selected at random - from a NN to prevent over-reliance on certain units
playground.tensorflow.org
Image Convolution: Applying a filter that adds each pixel value of an image to it's neighbours, weighted according to a kernel matrix.
                  - Extracts features from input. Output is a feature map
Pooling: reducing the size of an input by sampling from regions in the input
(1) Max-pooling: pooling by choosing the maximum value in each region
Convolutional NN: NN that uses convolution, usually for analyzing images (i.e., Image Convolution + Pooling in a NN)
                - Training is done to figure out what's the best filters for the input image
                - Models how human look at images
Input image -> [Convolution (calculates filters) -> Pooling (Summarize and reduces the size of inputs)] -> Flattening (Feeds into the inputs of NN)
[Convolution -> Pooling] can be applied multiple times before Flattening. Early layers of this to discover low-level features (Edges, Curves, Shapes); later layers to discover high-level features (objects, person, face, etc)
Note: In a convnet, the higher up a layer is, the more specialized it is. The first few layers in a convnet learn very simple and generic features, which generalize to almost all types of images. But as you go higher up, the features are increasingly specific to the dataset that the model is trained on.

Feed-forward NN: Good for classification. 1:1 mapping from input to output -Single-valued output.
Recurrent NN: Output from the NN is fed to it's input for future calculation. It maintains some states. Good for dealing with sequences of data, both input and/or output. N:N mapping from input to output - Output sequence of values.
              Example applications: Youtube to analyze videos, Google translator, etc
              One example is Long Short Memory Network
=========================================================================================================================================================================================================================================
File formats for training and evaluation.
For large data sets, load the data from data warehouse into SSD storage (Cloud or local). This will prevent underusage of resources, GPU/CPU during the traning/evaluation process.
JSONL: JSON Lines is a simple text-based format with rows. It is human readable and an ideal choice for small to medium-sized datasets.
TFRecord: Binary format and easier to read for computers, ideal for efficient training.
Parquet: Good for large and complex datasets.
=========================================================================================================================================================================================================================================
Data Augmentation:
(1) Image: replicate and distort/transform original data sets
(2) Voice: Add noisy background, audio on bad connection, etc.

Data Synthesis:
(1) Image OCR

Transfer Learning:
Example: A NN trained on 1 million images of diverse object types (animal, fruits, plants, people, etc). You want to come up with a NN for character OCR.
Make a copy of the NN, replace the output layer with a 10-unit layer. Then, there are 2 options to train the "new" NN:
(1) Only train output layer parameters - This works if having a small data set
(2) Train all parameters - This works with a huge data set.
The NN trained on 1 million images is called "supervised pretrained model".
The "new" copy of the NN is called "fine-tuning".
This works because layers of neurons have learnt different parts of the images. This could generalize to other types of images not found in the original dataset.
Only work for the same input types: text, images, audio.
Use-case exmaples: GPT-3, BERT, imageNet.

Skewed Datasets:
Precision = #True positives / (#True + #False positives) = #True positives / (#Total predicted positives)
Recall = #True positives / (#True positives + #False negatives) = #True positives / (#Total actual positives) - Helps detect if the learning algorithm is predicting negatives all the time because the value will be zero.
0 Precision and/or Recall is bad.
For use cases with skewed classes or a rare class, decently high precision and recall values helps reassures the usefulness of the learning algorithm.

Trade-off between Precision and Recall:
Logistic regression: 0 <= F(w,b) <= 1
Predict 1 if F(w,b) >= threshold
Predict 0 if F(w,b) < threshold
To predict 1 only if very confident, use high value of threshold. This results in high precision, low recall
To predict 1 even when in doubt, use low value of threshold. This results in low precision, high recall

Picking the threshold is not something can be done with cross-validation. It's up to the business needs / use-case.
To automatically decide on the best learning algorithm without having to manually select between Precision and Recall, choose the algorithm with the highest F1 score.
F1 score = 2 * PR / (P + R) <= Harmonic mean. A mean calculation which pays attention to the lower value.

Classification model evaluation metrics:
(1) Accuracy
(2) Area under ROC (Receiver Operating Characteristics) curve - binary classification models.
    - Comparison of a model's true-positive rate to false-positive rate at different classification thresholds.
    - The AUC metric tells you how well your model is at choosing between classes (for example, how well it is at deciding whether someone has heart disease or not). A perfect model will get an AUC score of 1.
(3) Confusion matrix
(4) Classification Report

Regression model evaluation metrics:
(1) R^2
(2) Mean absolute error (MAE)
(3) Mean squared error (MSE)

Tune model hyperparameters on Evaluation dataset.
3 ways to adjust hyperparameters:
(1) Manually
(2) Randomly with RandomSearchCV
(3) Exhaustively with GridSearchCV - Brute force, trying every single combination.

2 ways to save and load models:
(1) With Python pickle module.
(2) With joblib module
=========================================================================================================================================================================================================================================
Decision Tree Learning:
Decision 1: How to choose what feature to split on at each node?
            Goal: Maximize purity / Minimize impurity - decision which gives fastest path to leave nodes.
Decision 2: When do you stop splitting?
            (1) When a node is 100% one class
            (2) Tree depth. Deep tree could result in overfitting / high variance.
            (3) When improvement on purity score are below a threshold.
            (4) When the #samples in a node is below a threshold.
Entropy as a measure of impurity
p1 = Fraction of samples that are positive
p0 = 1 - p1
H(p1) = -p1log2(p1) - (1 - p1)log2(1 - p1)
      = -p1log2(p1) - p0log2(p0)
Note:
(1) Use log2 to make the symmetric bell curve range from 0 to 1
(2) 0log2(0) = 0 <- log(0) is negative infinity

Decision 1: Goal: Reduce entropy, H(p1) = Information gain.
Information Gain = H(p1root) - (wl * H(p1l) + wr * H(p1r))
wl = weight of left branch = #left samples / #root samples
wr = weight of right branch = #right samples / #root samples

Regression Tree: Decision Tree which predicts a number.
- Use variance of the sample labels to decide which features to split on.
- Split on feature which gives highest reduction in variance (highest information gain)
Information Gain = V(p1root) - (wl * V(p1l) + wr * V(p1r))

Any single decision tree is highly sensitive to small change in the data. Solution is to use tree ensemble to let multiple trees vote on the final prediction.
(1) Use sampling with replacement to build data for tree ensemble. This is called "bagged" decision tree (https://medium.com/data-science/understanding-sampling-with-and-without-replacement-python-7aff8f47ebe4)
(2) Random forest - Randomizing the feature selection. Select from k < n features at each node. k is usually sqrt(n). Works better for large feature set.
(3) XGBoost - Instead of picking from all examples with equal 1/m probability, make it more likely to pick misclassified samples from all the trees constructed so far.

Decision Trees vs. Neural Networks
Decision Trees and Tree Ensembles:
(1) Works well on structured data
(2) Not recommended for Unstrcutured data (video, audio, images, text)
(3) Fast to train
(4) Small decision trees maybe human interpretable.

NN:
(1) Works well on ALL types of data and the combination of them.
(2) Maybe slower to train compared to decision trees
(3) Works with transfer learning - Can use pretrained models to train the output layer with fewer data.
(4) Can string together multiple NN when building a system of multiple models working together.
=========================================================================================================================================================================================================================================
Collabortive Filtering: Recommend items to user based on ratings of other users who gave similar ratings as the user.
Content-based Filtering: Recommend items to user based on features of both the user and the items to find a good match.
  - R[i,j]=1 if user[j] has rated item[i]
  - Y[i,j] ratings of user[j] on item[i]
  User and item will have different sizes of features. Prediction can be simplified to numpy.dot(W[j], X[i]). bias doesn't affect the performance of the model much.
  For the calculation to work, need to transform the feature vectors to vectors of same dimension. Use NN to achieve this. Call it User NN and Item NN with the output layers having the same #neurons.
  These 2 NNs are trained as a single NN with ONE cost function J = sum((V[u,j] * V[m,i] - Y[i,j]) ** 2) + regularization for all R[i,j] = 1 (Use tf.keras.layers.Dot layer)
Recommendation from LARGE Catalogue - can be done with 2 steps: Retrieval and Ranking.
Retrieval:
(1) Generate last list of plausible item candidates:
    - For each item purchased by the user, find 10 most similar items. This can use overnight batch to calculate the min(|V[k] - V[i]| ** 2)
    - For most viewed 3 categories/genres, find top 10 items.
    - Top 20 items in the country/region.
(2) Combine the retrieved items into a list, removing duplicates and items already purchased.
Ranking:
(1) Use the NN to calculate predictions on the retrieved list. Only need to calculate the missing V[j] or V[i] for the final dot product. Can be done on separated NN. This should be quick.
(2) Display ranked items to the user.
To speed up the response times of the system, pre-compute vectors V[i] for ALL items that it might recommend. This can be done even before a user logs in or even before Xu or Vu vector is computed.
If 2 items have vectors V[i] and V[k] which are similar to one another, i.e., |V[k] - V[i]| is small, a conclusion could be drawn that they will be liked by similar users.
=========================================================================================================================================================================================================================================
Reinforement Learning
State action value function (Q-function)
Q(s,a) = Return if it, the agent:
(1) Start at state s
(2) Take action a (ONCE)
(3) Behave optimally after that: max(Q(s', a'))
The best possible return from state s is max(Q(s,a))
The best possible action in state s is the action which  gives max(Q(s,a))

Policy, pi(s) = a It's job is to take as input any state, s, and map it to some action that it wants the agent to take in that state.
Goal of Reinforcement learning: Find a policy, pi, which tells the agent what action (a = pi(s)) to take in every state (s) so as to maximize the return.
Bellman Equation: Q(s,a) = R(s) + gamma * max(Q(s', a'))
gamma is discount factor [0, 1]: smaller value means higher discount and makes the agent impatient, larger value makes it more patient to obtain reward at later steps. Usually close to 1. Example, 0.9, 0.99, etc.

In random/Stochastic environment, there will be multiple sequence of different rewards due to uncontrolled environment / misstep probabilities. A Policy execution could produce many different results.
In this case, choose a Policy (pi) which maximizes the average/expected sums of discounted rewards.
Bellman Equation: Q(s,a) = R(s) + gamma * E(max(Q(s', a'))); E = Average

In continuous state-space, s is a vector of numbers (some continuous numbers, some binary). Encode the actions (a) using one-hot feature vector.
Use Deep Reinforcement Learning to compute Q(s,a) at the current state (s). X = [s a]. In a state, s, use the NN to compute Q(s, a) of all possible actions to take. Then, pick the action which maximizes Q(s,a).
Use Bellman Equation to generate training datasets with Q(s,a) = X; R(s) + gamma * max(Q(s',a')) = Y

Learning Algorithm:
Input: [s, a]; Output: Q(s,a)
Initialize NN randomly as guess of Q(s,a).
Repeat {
  Take actions wuth the agent. Get (s,a,R(s),s') tuples. <= **
  Store 10,000 most recent tuples. (Replay Buffer)

  Train NN:
    Create training set of 10,000 samples using x = (s,a); y = R(s) + gamma * max(Q(s', a'))
    Train Qnew such that Qnew(s,a) ~= y (f_w_b(x) ~= y)
  Set Q = Qnew ***
}
Q will improve after every training. This is Deep Q-Learning/Q-Network and can be trained by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation.

To be more efficient, X = [s] and Y = Q(s, a) of all possible actions. Output layer has the number of neurons equal to #actions. With this a single inference will generate all the possible Q(s,a) with all possible actions. Then, the max(Q(s',a')) term is readily available.

Using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. Here is why:
Bellman equation error: Y - Q(s,a;w) = R(s) + gamma * max(Q(s', a'; w)) - Q(s,a; w). 
Notice that Y will keep changing in every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create a separate neural network for generating the Y targets. We call this separate neural network the target Q-Network and it will have the same architecture as the original Q.
Steps:
Every C time steps we will use the Q-target-Network to generate the Y targets and update the weights of the Q-target-Network using the weights of the ð‘„-Network. We will update the weights of the the Q-target-Network using a soft update.


To avoid instabilities, use a Target Network and Experience Replay.



** How to choose actions while still learning? 2 options:
(1) Pick the action a which maximizes Q(s,a) - Because of random initialization, the NN might get stuck in it's own misconception that some actions (a) are *always* bad and therefore, would never get a chance to learn them well.
(2) epsilon-greedy policy (e = 0.05)
    (i) Greedy/Exploitation: With probabiity 0.95, Pick the action a which maximizes Q(s,a)
    (ii) Exploration: With probability 0.05, pick an action randomly.
    Start e high (1.0) and gradually decreases it to 0.01 in the course of the training.

Mini-batch helps in both Supervised and Reinforcement Learning when datasets is large and it takes every iteration to compute sum((f_w_b(X) - y) ** 2)/2m over large datasets and therefore takes very long time to converge and computationally expansive. However, mini-batch will exhibit some unreliable behaviour ("noisy") in graident descent although eventually it will converge faster compared to usual batch learning.
*** To make the NN converge more reliably, soft-update learning will help:
(1) Prevent abrupt changes to the NN
(2) Prevent one worse learning step overwrite the existing better NN
Q(w,b) = Qnew(w_new, b_new)
W = 0.01 * Wnew + 0.99 * W
B = 0.01 * Bnew + 0.99 * B
0.01 and 0.99 adds up to 1 and are the hyperparameters which control how aggressively or gradually the training updates the NN parameters.

In machine learning, logits are the raw, unnormalized scores or outputs from a neural network's final layer before an activation function like softmax or sigmoid is applied.
Binary Classification uses Sigmoid activation function and BinaryCrossEntropy loss function.
Multi-class Classification uses Softmax activation function and CategoricalCrossEntropy loss function.