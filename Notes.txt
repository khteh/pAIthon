Conditional Probability:
P(a|b) = P(a ^ b) / P(b)
P(a ^ b) = P(a|b)P(b)
P(a ^ b) = P(b|a)P(a)

P(a|b)P(b) = P(b|a)P(a)
P(a|b) = P(b|a)P(a) / P(b) <= Bayes' Rule
P(b|a) = P(a|b)P(b) / P(a) <= Bayes' Rule

Bayes' Rule: 
Knowing - P(visible effect|unknown cause)
We can conclude - P(unknown cause | visible effect)

Example:
Knowing - P(medical test result | disease)
We can conclude - P(disease | medical test result)

If Probability of 'a' and 'b' are independent,
P(a ^ b) = P(b)P(a)

P(a∣b) = P(b∣a)P(A) / P(B)

Bayes rule/formula:
Prior Odds: 5:95
Likelihood Ratio: P(A|B) / P(A|C) : (80/100) / (10/100) = (80/100) * (100/10) = 80/10 = 8
Posterior Odds: 40:95

Joint Probability:
P(A|b) = P(A, b) / P(b) : Comma signifies ^
       = alpha * P(A, b), where alpha = 1/P(b) which is just some constant.
       = alpha * <Probability distribution of 'b' which sums up to 1>
=> Conditional probability is proportional to Joint probability

Inclusion-Exclusion:
P(a v b) = P(a) + P(b) - P(a^b)

Marginalization: Calculate probability of independent variable based on given/known joint probabilities.
P(a) = P(a,b) + P(a, Not(b))
P(X = xi) = Summation over all the values (j) y could take on of P(X = xi, Y = yj)

Conditioning: Calculate probability of independent variable based on given/known conditional probabilities.
P(a) = P(a|b)P(b) + P(a|Not(b))P(Not(b))
P(X = xi) = Summation over all the values (j) y could take on of P(X = xi|Y = yj)P(Y = yj)

Bayesian Network: Data structure which represents the dependencis among random variables.
- Directed graph
- Each node represents a random variable.
- Arrow from X to Y means X is a parent of Y
- Each node X has a probability distrubution P(X | Parents(X))

Inference:
- Query X: variable for which to compute distribution
- Evidence variables E: observed variables for event e
- Hidden variables Y: non-evidence and non-query variable
- Goal: Calculate P(X | e) where e could be more than 1.

Inference by Enumeration:
P(X | e) = alpha * P(X,e) = alpha * Summation over all the values (j) y could take on of P(X, e, y)
X: query variable
e: evidence
y: ranges over values of hidden variables
alpha: normalizes the result.
Cons: Time complexity grows with complexity of the network

Aproximate Inference:
- Sampling:
  - Rejection Sampling: Filter out samples which do not match event specification
  - Likelihood Weighting:
    - Start by fixing the values of evidence variables.
    - Sample the non-evidence variables using conditional probabilities in the Bayesian Network
    - Weight each sample by its likelihood: the probability of all of the evidence.

Markov Assumption: The assumption that the current state depends on only a finite fixed number of previous states.
Markov Chain: A sequence of random variables where the distribution of each variable follows the Markov assumption.
Hidden Markov Model: a Markov model for a system with hidden states which generate some observed events.
Sensor Markov Assumption: the assumption that the evidence variable depends only on corresponding state.

Supervised Learning > Classification:
Perceptron Learning Rule: Given data point (x,y), update each weight according to: w =  w + alpha*(y - h(X)) * x
Start with random weights, learn from data and update the weights that result in better weight vector which reduces loss.

Support Vector Machines:
  Maximum Margin Separator: Boundary that maximizes the distance between any of the data points
If the data is not linearly separable, it works from higher dimension to find the separation boundary. For example, other shapes than linear lines.

Optimization Problem Formulation:
- Local Search: 
  - Algorithms: Hill-Climbing, Simulated Annealing
- Linear Programming
  - Algorithms: 
- Constraint Satisfaction
  - Algorithms: AC3, Backtracking

Supervised Learning > Regression:
Learning a function mapping an input point to a continuous value.

Types of loss functions:
0-1 Loss: Used in discrete classification
L1 Loss: |actual - prediction| -> Used in continuous number prediction. Used when we don't care about outliers.
L2 Loss: (actual - prediction)^2 -> Penalizes worse / bigger loss more harshly. Used when we care about outliers.

Underfitting (High bias):
- J(train) AND J(cv) are high
- The model has a strong preconception of how the data  should fit into itself. For example, linear function, etc.
- More data will NOT help.

Overfitting (High variance):
- J(train) is low; J(cv) is high
- A model that fits too closely to a particular data set and therefore may fail to generalize to future data. High variance because if the model overfits, a tiny change in one of the training data set will end up completely different model.
- This is a side effect of minimizing loss of a model. If loss = 0, the model may only work on the specific data set.
- One way to counter this problem is add other parameters to optimization. For example, consider complexity:
- More data is likely to help.

High bias AND high variance:
- J(train) is high; J(cv) higher than J(train)
- Sometimes happen in NN. Less in linear regression.
- Doesn't really happen for linear models applied to 1-D.
- Underfit and overfit on parts of the data at the same time.

Cost(h) = Loss(h) + w*Complexity(h) : 'w' gives weight to the complexity

Adding the term w*Complexity(h) is called "Regularization": Penalizing hypotheses that are more complex to favour simpler, more general hypotheses
To overcome overfitting:
(1) Collect more training samples / observations.
(2) Use fewer features.
Many features + insufficient data -> overfitting
(3) Regularization - Gently reduces the impact of some of the features without eliminating them outright. It encourages the model to shrink the values of the parameters without necessarily setting them to 0 which eliminates the features.
    Cost(w,b) = (sum((f_w_b(x) - y) ** 2) + lambda * sum(w[j] ** 2)) / 2m
    f_w_b <= Linear regression modal
    sum((f_w_b(x) - y) ** 2) <= Mean squared error. Fit data
    sum(w[j] ** 2) <= Regularization term. Keep w[j] small
    lambda balances both goals
    Example: f_w_b = w1x + w2x^2 + w3x^3 + w4x^4 + b
    lambda = 0: Overfitting. Wiggly curve/graph which tries to fit every single data point in the training data set.
    lambda = 10^10: f_w_b = b Underfitting
Gradient Descent withh regularization:
w = w - alpha/m * (sum((f_w_b(x) - y) * x) + lambda * w[j])
  = w * (1 - alpha * lambda / m) - (alpha / m ) * sum((f_w_b(x) - y) * x)
  alpha: 0.01, lambda: 10, m: 100 => (1 - alpha * lambda / m) = (1 - 0.001) = 0.999
  This shows that every iteration of gradient descent, regularization reduces the value of w, thus reduces the impact of the feature.

Strategy to debugging a learning algorithm:
(1) Get more data - Fixes high variance.
(2) Try smaller sets of features (Simplify polynomial equation by removing some of the higher order components.) - Fixes high variance.
(3) Add more features - Fixes high bias
(4) Add polynomial features - Fixes high bias
(5) Decrease lambda - Fixes high bias
(6) Increase lambda - Fixes high variance.

Hold-out cross validation splits data into training and testing data sets. How to split?
k-fold corss-validation: Splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data as training set.

Feature Scaling: When a feature has a large range of values, the weight/parameter calculated by the model will have small range. In this case, a small change in weight will have big change in the cost value.
This causes the gradient descent to take longer time to find it's global minimum, i.e., converge slower. To oversome this, scale the input data. 2 ways:
(1) Divide by the maximum value of the data range. This will produce scaled input data of [0, 1]
(2) Mean normalization: Find the average value (ave). (x - ave) / (Xmax - Xmin). This will produce scaled input data of [-1, 1] which centers around 0.
(3) Z-score normalization: Find the standard deviation (d) and the mean. (x - ave) / d.
    Mean: sum(data) / len(data)
    standard deviation: sum((data[i] - mean) ** 2) / len(data)
Rules of thumb: Aim for [-1, 1] input data range. These need to rescale:
(1) [-100 , 100] : Too large
(2) [-0.001, 0.001] : Too small
(3) [98.6, 105]: Too large. Example, Fahrenheight body temperature.

Reinforcement Learning:
Given a set of rewards or penalties, learn what actions to take in the future.
Markov Decision Process:
Model for decision-making, representing states, actions, and their rewards.
Q-Learning:
Method for learning a function Q(s,a), estimate of the value (reward) of performing action 'a' in state 's'. Start with Q(s,a) = 0 for all s,a.
Q(s,a) <- Q(s,a) + w*(new estimate - old estimate): w:0 old values are more important; w:1 new values are more important.
Q(s,a) <- Q(s,a) + w*((r + future reward estimate) - Q(s,a)) 
Q(s,a) <- Q(s,a) + w*((r + w1*max(Q(s',a') for all a')) - Q(s,a)): w1 provides weights for future vs current reward

Balance between Exploration and Exploitation.

Function Approximation:
Approximating Q(s,a), often by a function combining various features, rathen than storing one value for every state-action pair.
Similar to depth-limiting approach in MiniMax. Used when it is not feasible to explore all the posible values of Q(s,a) in a bigger state space.

Unsupervised Learning:
Given input data without any additional feedback (label), learn patterns, find structure in the data.
Tasks:
(1) Clustering: Organize a set of objects into groups in such a way that similar objects tend to be in the same group. Example: Genetic research, Image segmentation, Market research, Medical imaging, Social network analysis.
    One technique is k-means clustering: Algorithm for clustering data based on repeatedly assigning points to clusters (k number of clusters) and updating those clusters' centers
(2) Anomaly Detection: Find unusual events in the data.
    Use case: Fraud detection.
(3) Dimensionality Reduction: Compress large corpuses of data to a much smaller dataset without losing key / important information.

k-means clustering:
(1) Choose a number for 'k' cluster centroids, u[k]. k < len(data)
(2) Randomly assigns 'k's values which have the same dimension as the dataset.
(3) Assign samples to the nearest centroid (k)
    for i of range(len(data)):
      c[i] = index (from 1 to k) of cluster centroids closest to x[i]
    The distance can be calculated with min(|x[i] - u[k]|), or more generally, min(|x[i] - u[k]|  ** 2 )
(4) Move the cluster centroids to the average of the points
    for i of range(k):
      u[i] = average (mean) of points assigned to cluster k
    The means can be calculated with sum(x[]) / len(x[])
There will be situations where one of the uk has zero data points assigned to it. In this case, either
(1) Remove the empty cluster: k = k - 1
(2) Reinitialize the cluster centroids and restart the algorithm.
Optimization objective:
c[i] = index of cluster (1,2,...,k) to which sample x[i] is currently assigned to.
uk = cluster centroid k
uc[i] = location of centroid k to which x[i] is currently assigned to.
cost function: J(c, uk) = sum(|x[i] - u[k]|  ** 2 ) / m
Every step along the way reduces J(c, uk).
To minimize local minimum and choose the best cluster centroids, use multiple random initializations and choose the cluster with the lowest J(c,u). i.e., run the algorithm multiple times, maybe 50 to 1000.

Anomaly Detection:
Density Estimation: Model p(x) from data. Probability of x[i] being seen in the dataset.
p(x[i]) < epsilon : Anomaly
p(x[i]) >= epsilon: OK
p = Gaussian distribution - Needs mean and sigma (standard distribution); sigma ** 2 is variance
To model p(x) with multiple featues (N), every feature is a Gaussian distribution.
(1) Calculate the mean and sigma of every feature.
(2) p(x) = p(x1; u1,s1) * p(x2; u2,s2) * p(x3; u3,s3) * ... * p(xN; uN,sN)

Anomaly Detection vs Supervised Learning:
Anomaly Detection is mostly used when:
(1) Very small number of positive exaples (y=1). 0-20 is common. Large number of negative (y=0)
(2) Many different "types" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous samples seen so far (training set)
Examples: Security related applications like fraud, defects detection of manufacturing very complex products like the aircraft engines, monitoring complex systems like in a data center.

Supervised Learning:
(1) Large number of positive and negative examples
(2) Enough positive examples for algorithm to get a sense of what positive examples are lke; future positive samples likely to be similar to ones in training set.
Examples: Email spam classification, defects detection of mobile phone manufacturing, weather prediction, disease classification.

Anomaly Detection feature selection:
(1) Choose feature with gaussian distribution property (plt.hist(x)). Otherwise, try to transform it:
    (i) log(x + C) : Lower values of C transform better
    (ii) sqrt(x)
    (iii) x ^ 1/3

Anomaly Detection error analysis:
Most common problem: p(x) is comparable for both normal and anomalous samples. p(x) is large. In this case, the algorithm will fail to flag an anomalous test data. Solutions:
(1) Check the anomalous test data. Create new feature which helps detect the anomaly.
(2) Create new feature based on existing ones using simple equations like x1 / x2 or (x1^2) / x2, etc.

Neural Network (multilayer perceptron):
Activation functions:
(1) Step function: g(x) = 1 if x >=0, else 0
(2) Logistic Sigmoid: g(x) = e^x / (1 + e^x)
(3) Rectifier Linear Unit (ReLu): g(x) = max(0,x)
Choices of Activation Function:
(1) Output layer: Depends on the nature of the label:
    (i) Binary classification: Sigmoid
    (ii) -/+: Linear. For example, stock price change 
    (iii) Only positive numbers: ReLU
(2) Hidden layers: Mostly use LeLU. Reasons:
    (i) Simpler calculation compared to Sigmoid. This makes NN runs faster.
    (ii) There are 2 horizontal asymtotes in Sigmoid. This results in derivaties being close to 0 and therefore make gradient descent slower to run to global minimum.
    Do NOT use linear activation function. This will result in a linear regression and defeats the purpose of NN.
    The "off" or disable feature  of the ReLU activation enables models to stitch together linear segments to model complex non-linear functions.
Types of Layers:
(1) Dense: Every neuron in a layer computes it's output / activation using ALL inputs/activations from ALL neurons in previous layer.
(2) Convolution: Neurons only look at regions of the data. Benefits:
    (i) Faster computation.
    (ii) Need less training data - less prone to overfitting.
    
h(x1,x2) = g(w0 + w1x1 + w2x2 + ...)
Example:
(1) To mdel the OR function:
    h(x1,x2) = g(-1 + x1 + x2); w0 = -1, w1=w2=1
(2) To mdel the AND function:
    h(x1,x2) = g(-2 + x1 + x2); w0 = -2, w1=w2=1
Training - to calculate the parameters:
- Trade-offs between speed and accuracy
(1) Gradient Descent: Algorithm for minimizing loss when training NN
    Start with a random choice of weights, repeat:
    - Calculate the gradient based on ALL data points (training data set): direction that will lead to decreasing loss
    - Update weights according to the gradient
(2) Stochastic Gradient Descent: Same as Gradient Descent but "... based on ONE data point"
(3) Mini-Batch Gradient Descent: Same as Gradient Descent but "... based on ONE SMALL BATCH of data points"
Perceptron: Only capable of learning linearly separable decision boundary
Multilayer NN: Artificial NN with an input layer, an output layer, and at least one hidden layer. Capable of modelling more complex problems compared to perceptron.
To train multi-layer NN, we have to propagate the loss/error from the output back to the hidden layers:
Backpropagation: Algorithm for training NN with hidden layers
    Start with a random choice of weights, repeat:
    - Calculate error for output layer
    - For each layer, starting with output layer, and moving inwards towards the earliest hidden layer
      - Propagate error back one layer

To prevent underfitting (high bias):
(1) Use a larger network.

To prevent Overfitting (high variance):
(1) Collect more data
(2) Choose lambda appropriately.
Dropout: Temporarily removing units - selected at random - from a NN to prevent over-reliance on certain units
playground.tensorflow.org
Image Convolution: Applying a filter that adds each pixel value of an image to it's neighbours, weighted according to a kernel matrix.
                  - Extracts features from input. Output is a feature map
Pooling: reducing the size of an input by sampling from regions in the input
(1) Max-pooling: pooling by choosing the maximum vallue in each region
Convolutional NN: NN that uses convolution, usually for analyzing images (i.e., Image Convolution + Pooling in a NN)
                - Training is done to figure out what's the best filters for the input image
                - Models how human look at images
Input image -> [Convolution (calculates filters) -> Pooling (Summarize and reduces the size of inputs)] -> Flattening (Feeds into the inputs of NN)
[Convolution -> Pooling] can be applied multiple times before Flattening. Early layers of this to discover low-level features (Edges, Curves, Shapes); later layers to discover high-level features (objects, person, face, etc)

Feed-forward NN: Good for classification. 1:1 mapping from input to output -Single-valued output.
Recurrent NN: Output from the NN is fed to it's input for future calculation. It maintains some states. Good for dealing with sequences of data, both input and/or output. N:N mapping from input to output - Output sequence of values.
              Example applications: Youtube to analyze videos, Google translator, etc
              One example is Long Short Memory Network
=========================================================================================================================================================================================================================================
File formats for training and evaluation.
For large data sets, load the data from data warehouse into SSD storage (Cloud or local). This will prevent underusage of resources, GPU/CPU during the traning/evaluation process.
JSONL: JSON Lines is a simple text-based format with rows. It is human readable and an ideal choice for small to medium-sized datasets.
TFRecord: Binary format and easier to read for computers, ideal for efficient training.
Parquet: Good for large and complex datasets.
=========================================================================================================================================================================================================================================
Data Augmentation:
(1) Image: replicate and distort/transform original data sets
(2) Voice: Add noisy background, audio on bad connection, etc.

Data Synthesis:
(1) Image OCR

Transfer Learning:
Example: A NN trained on 1 million images of diverse object types (animal, fruits, plants, people, etc). You want to come up with a NN for character OCR.
Make a copy of the NN, replace the output layer with a 10-unit layer. Then, there are 2 options to train the "new" NN:
(1) Only train output layer parameters - This works if having a small data set
(2) Train all parameters - This works with a huge data set.
The NN trained on 1 million images is called "supervised pretrained model".
The "new" copy of the NN is called "fine-tuning".
This works because layers of neurons have learnt different parts of the images. This could generalize to other types of images not found in the original dataset.
Only work for the same input types: text, images, audio.
Use-case exmaples: GPT-3, BERT, imageNet.

Skewed Datasets:
Precision = #True positives / (#True + #False positives) = #True positives / (#Total predicted positives)
Recall = #True positives / (#True positives + #False negatives) = #True positives / (#Total actual positives) - Helps detect if the learning algorithm is predicting negatives all the time because the value will be zero.
0 Precision and/or Recall is bad.
For use cases with skewed classes or a rare class, decently high precision and recall values helps reassures the usefulness of the learning algorithm.

Trade-off between Precision and Recall:
Logistic regression: 0 <= F(w,b) <= 1
Predict 1 if F(w,b) >= threshold
Predict 0 if F(w,b) < threshold
To predict 1 only if very confident, use high value of threshold. This results in high precision, low recall
To predict 1 even when in doubt, use low value of threshold. This results in low precision, high recall

Picking the threshold is not something can be done with cross-validation. It's up to the business needs / use-case.
To automatically decide on the best learning algorithm without having to manually select between Precision and Recall, choose the algorithm with the highest F1 score.
F1 score = 2 * PR / (P + R) <= Harmonic mean. A mean calculation which pays attention to the lower value.
=========================================================================================================================================================================================================================================
Decision Tree Learning:
Decision 1: How to choose what feature to split on at each node?
            Goal: Maximize purity / Minimize impurity - decision which gives fastest path to leave nodes.
Decision 2: When do you stop splitting?
            (1) When a node is 100% one class
            (2) Tree depth. Deep tree could result in overfitting / high variance.
            (3) When improvement on purity score are below a threshold.
            (4) When the #samples in a node is below a threshold.
Entropy as a measure of impurity
p1 = Fraction of samples that are positive
p0 = 1 - p1
H(p1) = -p1log2(p1) - (1 - p1)log2(1 - p1)
      = -p1log2(p1) - p0log2(p0)
Note:
(1) Use log2 to make the symmetric bell curve range from 0 to 1
(2) 0log2(0) = 0 <- log(0) is negative infinity

Decision 1: Goal: Reduce entropy, H(p1) = Information gain.
Information Gain = H(p1root) - (wl * H(p1l) + wr * H(p1r))
wl = weight of left branch = #left samples / #root samples
wr = weight of right branch = #right samples / #root samples

Regression Tree: Decision Tree which predicts a number.
- Use variance of the sample labels to decide which features to split on.
- Split on feature which gives highest reduction in variance (highest information gain)
Information Gain = V(p1root) - (wl * V(p1l) + wr * V(p1r))

Any single decision tree is highly sensitive to small change in the data. Solution is to use tree ensemble to let multiple trees vote on the final prediction.
(1) Use sampling with replacement to build data for tree ensemble. This is called "bagged" decision tree (https://medium.com/data-science/understanding-sampling-with-and-without-replacement-python-7aff8f47ebe4)
(2) Random forest - Randomizing the feature selection. Select from k < n features at each node. k is usually sqrt(n). Works better for large feature set.
(3) XGBoost - Instead of picking from all examples with equal 1/m probability, make it more likely to pick misclassified samples from all the trees constructed so far.

Decision Trees vs. Neural Networks
Decision Trees and Tree Ensembles:
(1) Works well on structured data
(2) Not recommended for Unstrcutured data (video, audio, images, text)
(3) Fast to train
(4) Small decision trees maybe human interpretable.

NN:
(1) Works well on ALL types of data and the combination of them.
(2) Maybe slower to train compared to decision trees
(3) Works with transfer learning - Can use pretrained models to train the output layer with fewer data.
(4) Can string together multiple NN when building a system of multiple models working together.
